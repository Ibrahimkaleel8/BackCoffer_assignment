{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9312f26",
   "metadata": {},
   "source": [
    "# Data scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef4785ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac2f2459",
   "metadata": {},
   "outputs": [],
   "source": [
    "req = requests.get(\"https://insights.blackcoffer.com/ai-human-robotics-machine-future-planet-blackcoffer-thinking-jobs-workplace/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45f9fa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(req.content, \"html.parser\")\n",
    "res = soup.title\n",
    "paras = soup.find_all('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbb256b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It’s the year 2060. An automaton in a Research Laboratory says to a Scientist, “Warning! Error Occurred Reformatting Hard Disk Now!” The scientist panics. Automaton says again,” Ha! Ha! Just Kidding! “. Funny Right. Before some of you say that this joke isn’t realistic, “How can an Automaton tell you a joke?” But what if I tell you in 2017, “Sofia” the robot made a joke on the show Good Morning Britain! Who thought computers could tell us a joke? Hard to believe? Well, the idea of giving computers human-enjoy thinking has now become a reality. Thanks to the technological advancement in AI in the last decade. Before diving deep into how AI can impact the future of work, let’s begin with the simple question: what’s AI? Artificial Intelligence provides machines the power to think from data. The machine uses the patterns and trends found in data and makes its decision, but cannot create thought beyond these patterns and trends. With the rise of AI, humans are divided into one question. Are machines human’s friend or foe? Tech executives and politicians on conference stages, campaign rallies, and even science fiction Hollywood movies like Carbon Black, Westworld, Minority Report, and Ex Machina have given their take on this question. Some believe AI will help us solve problems while others believe that the rise of AI will result in destruction and maybe the end of the world, we all know. Stephen Hawking made it no secret of his concern about the rise of superhuman AI that eventually would escape earth to a new planet. No, this isn’t a plot of Black Mirror. Right now, Superhumans may not be a reality, but AI is. “Homo Deus”, the emergence of the new Digital God using AI. God must also worry, as AI might take his job. Here’s some Career Advice, have you thought about being a Robot? The fear that AI would automate all jobs in the future eventually leaving all humans jobless has been daunting for many workers today. Statistics show that nearly 37% of workers worry about losing their jobs to robots. While another thought that many people believe is that though the rise of AI with result in automating most of the jobs in the future, however, it also will create millions of new job opportunities. AI is already replacing most manual and repetitive tasks. For example, buying a metro ticket or a movie ticket is now almost a human-less interaction. Each year the number of industrial robot jobs increases by 14 %. At this rate, it’s predicted that the 20 million jobs in the manufacturing industry will be replaced by robots due to automation. The coronavirus pandemic and recession have boosted the demand for automation. The Robotic Process Automation (RPA) Software industry has experienced an increase of 19.53% in the year 2021. Coronavirus pandemic has increased interest in technology that reduces human contact as minimal for making workplaces safe. Our workplaces will look much different in the next five to ten years. AI will help humans in simplifying repetitive processes. The two most important catalysts for the future of work are the two D’s- Digitization and Datafication. Digitalization is converting data to digital formats (computer-readable). For example, text to Html, analog video to YouTube video. Digitization helps in increasing data exponentially. Datafication is quantifying human life to data and improving the data-driven business model. By 2025, it is forecasted that the digital transformation space will build in a $3,294 billion industry! One thing is clear, no data, no future of work. What we find is that the future of data and the future of work will go hand in hand. The total volume of data in the datasphere that is created, captured, copied, and consumed in the world is predicted to reach 175 zettabytes by 2025. To give you a much better picture for understanding, if we represent the digital universe as stacks of tablets, there would be 27.25 stacks from earth to the moon. It’s time to prepare for the data-dominated future as Industry 4.0/Fourth Industrial Revolution has begun. So, let’s see how artificial intelligence will affect the following fields: Human Resource: Nowadays, recruiters use AI-powered tools for hiring workers. Using these tools, recruiters get insights into a candidate’s skills, personality and even check whether the candidate is fit for the organization. For example, the company AllyO first identifies high-potential candidates through assessment and smart screening, and then automatically schedules interviews using AI. HR departments at large companies receive hundreds of resumes for a job opening. Entry-level roles focusing on screening and scheduling will be automated. AI will automate specific HR jobs, not HR roles. A Deloitte study found that AI has already eliminated 800,000 low-skilled jobs in the UK, but 3.5 million new jobs were also created. Roles that focus on complex decisions like resolving disputes within a department will continue to be a very human endeavor. Finance and Accounting: In 2015, a report from Accenture named “Finance 2020: death by digital” predicted that 40 percent of transactional accounting work would be automated by 2020. Has technology replaced the human factor? Well, AI has created new jobs involving managing the AI system and using the information to create insights. For example, accounting software has already automated bookkeeping tasks that used to be done by humans, but that’s only opened the door for former bookkeepers to learn skills needed to run and manage the software for employers and clients. Advisors are another crucial role of the accounting and finance team. Using the information gained from transactions in books, the team creates insights to improve business strategy. Owing to automation, the team spends more time analyzing numbers. Marketing and Sales: Marketing automation has helped companies strategize the proper utilization of the company’s resources, managing time, and achieving budget targets. Marketing automation has helped to draw conclusions at a scale no marketer ever would. In this process, marketers and machines both excel in different parts. Marketers using AI tools drive more conversions in less time. Human Intelligence with technology can help identify the right customers to talk to and at the right time. Modern Marketers understand the insights from any marketing campaign and create it into effective messaging. Engineering: Technology is changing in a blink of an eye. The technologies used five years ago in the industry have become obsolete today. Engineers will have to keep up with the technological advancements and keep upgrading their skills to stay relevant in the industry. Learning to work alongside machines and designing work such that interaction better humans and machines are better are going to be important skills for engineers in the future.  In the 18th and 19th centuries, the rise of the industrial revolution centuries led to millions of people losing their jobs because of scientific advancements. But that also ended in creating millions of other jobs. Statisticians have said, when automation destroys jobs, people find new ones. Thus, AI holds a more optimistic picture for the future. In the future, AI is not going to replace humans, rather make jobs more humane. AI will disrupt millions of middle and entry-level jobs in the next few years but will also create millions of additional jobs and help to boost economies.\n"
     ]
    }
   ],
   "source": [
    "texts = \" \".join([paragraph.text.strip() for paragraph in paras])\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e79f5f60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7452"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8f1fcc",
   "metadata": {},
   "source": [
    "Removing Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5285c8a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7452"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "texts = re.sub(r'[\\'\\“\\”\\()\\%\\,\\-\\'\\’\\?\\ ]', ' ', texts)\n",
    "texts[0:200]\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fafe7be",
   "metadata": {},
   "source": [
    "# Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "089b8663",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "280876b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It',\n",
       " 's',\n",
       " 'the',\n",
       " 'year',\n",
       " '2060',\n",
       " '.',\n",
       " 'An',\n",
       " 'automaton',\n",
       " 'in',\n",
       " 'a',\n",
       " 'Research',\n",
       " 'Laboratory',\n",
       " 'says',\n",
       " 'to',\n",
       " 'a',\n",
       " 'Scientist',\n",
       " 'Warning',\n",
       " '!',\n",
       " 'Error',\n",
       " 'Occurred',\n",
       " 'Reformatting',\n",
       " 'Hard',\n",
       " 'Disk',\n",
       " 'Now',\n",
       " '!',\n",
       " 'The',\n",
       " 'scientist',\n",
       " 'panics',\n",
       " '.',\n",
       " 'Automaton',\n",
       " 'says',\n",
       " 'again',\n",
       " 'Ha',\n",
       " '!',\n",
       " 'Ha',\n",
       " '!',\n",
       " 'Just',\n",
       " 'Kidding',\n",
       " '!',\n",
       " '.',\n",
       " 'Funny',\n",
       " 'Right',\n",
       " '.',\n",
       " 'Before',\n",
       " 'some',\n",
       " 'of',\n",
       " 'you',\n",
       " 'say',\n",
       " 'that',\n",
       " 'this',\n",
       " 'joke',\n",
       " 'isn',\n",
       " 't',\n",
       " 'realistic',\n",
       " 'How',\n",
       " 'can',\n",
       " 'an',\n",
       " 'Automaton',\n",
       " 'tell',\n",
       " 'you',\n",
       " 'a',\n",
       " 'joke',\n",
       " 'But',\n",
       " 'what',\n",
       " 'if',\n",
       " 'I',\n",
       " 'tell',\n",
       " 'you',\n",
       " 'in',\n",
       " '2017',\n",
       " 'Sofia',\n",
       " 'the',\n",
       " 'robot',\n",
       " 'made',\n",
       " 'a',\n",
       " 'joke',\n",
       " 'on',\n",
       " 'the',\n",
       " 'show',\n",
       " 'Good',\n",
       " 'Morning',\n",
       " 'Britain',\n",
       " '!',\n",
       " 'Who',\n",
       " 'thought',\n",
       " 'computers',\n",
       " 'could',\n",
       " 'tell',\n",
       " 'us',\n",
       " 'a',\n",
       " 'joke',\n",
       " 'Hard',\n",
       " 'to',\n",
       " 'believe',\n",
       " 'Well',\n",
       " 'the',\n",
       " 'idea',\n",
       " 'of',\n",
       " 'giving',\n",
       " 'computers',\n",
       " 'human',\n",
       " 'enjoy',\n",
       " 'thinking',\n",
       " 'has',\n",
       " 'now',\n",
       " 'become',\n",
       " 'a',\n",
       " 'reality',\n",
       " '.',\n",
       " 'Thanks',\n",
       " 'to',\n",
       " 'the',\n",
       " 'technological',\n",
       " 'advancement',\n",
       " 'in',\n",
       " 'AI',\n",
       " 'in',\n",
       " 'the',\n",
       " 'last',\n",
       " 'decade',\n",
       " '.',\n",
       " 'Before',\n",
       " 'diving',\n",
       " 'deep',\n",
       " 'into',\n",
       " 'how',\n",
       " 'AI',\n",
       " 'can',\n",
       " 'impact',\n",
       " 'the',\n",
       " 'future',\n",
       " 'of',\n",
       " 'work',\n",
       " 'let',\n",
       " 's',\n",
       " 'begin',\n",
       " 'with',\n",
       " 'the',\n",
       " 'simple',\n",
       " 'question',\n",
       " ':',\n",
       " 'what',\n",
       " 's',\n",
       " 'AI',\n",
       " 'Artificial',\n",
       " 'Intelligence',\n",
       " 'provides',\n",
       " 'machines',\n",
       " 'the',\n",
       " 'power',\n",
       " 'to',\n",
       " 'think',\n",
       " 'from',\n",
       " 'data',\n",
       " '.',\n",
       " 'The',\n",
       " 'machine',\n",
       " 'uses',\n",
       " 'the',\n",
       " 'patterns',\n",
       " 'and',\n",
       " 'trends',\n",
       " 'found',\n",
       " 'in',\n",
       " 'data',\n",
       " 'and',\n",
       " 'makes',\n",
       " 'its',\n",
       " 'decision',\n",
       " 'but',\n",
       " 'can',\n",
       " 'not',\n",
       " 'create',\n",
       " 'thought',\n",
       " 'beyond',\n",
       " 'these',\n",
       " 'patterns',\n",
       " 'and',\n",
       " 'trends',\n",
       " '.',\n",
       " 'With',\n",
       " 'the',\n",
       " 'rise',\n",
       " 'of',\n",
       " 'AI',\n",
       " 'humans',\n",
       " 'are',\n",
       " 'divided',\n",
       " 'into',\n",
       " 'one',\n",
       " 'question',\n",
       " '.',\n",
       " 'Are',\n",
       " 'machines',\n",
       " 'human',\n",
       " 's',\n",
       " 'friend',\n",
       " 'or',\n",
       " 'foe',\n",
       " 'Tech',\n",
       " 'executives',\n",
       " 'and',\n",
       " 'politicians',\n",
       " 'on',\n",
       " 'conference',\n",
       " 'stages',\n",
       " 'campaign',\n",
       " 'rallies',\n",
       " 'and',\n",
       " 'even',\n",
       " 'science',\n",
       " 'fiction',\n",
       " 'Hollywood',\n",
       " 'movies',\n",
       " 'like',\n",
       " 'Carbon',\n",
       " 'Black',\n",
       " 'Westworld',\n",
       " 'Minority',\n",
       " 'Report',\n",
       " 'and',\n",
       " 'Ex',\n",
       " 'Machina',\n",
       " 'have',\n",
       " 'given',\n",
       " 'their',\n",
       " 'take',\n",
       " 'on',\n",
       " 'this',\n",
       " 'question',\n",
       " '.',\n",
       " 'Some',\n",
       " 'believe',\n",
       " 'AI',\n",
       " 'will',\n",
       " 'help',\n",
       " 'us',\n",
       " 'solve',\n",
       " 'problems',\n",
       " 'while',\n",
       " 'others',\n",
       " 'believe',\n",
       " 'that',\n",
       " 'the',\n",
       " 'rise',\n",
       " 'of',\n",
       " 'AI',\n",
       " 'will',\n",
       " 'result',\n",
       " 'in',\n",
       " 'destruction',\n",
       " 'and',\n",
       " 'maybe',\n",
       " 'the',\n",
       " 'end',\n",
       " 'of',\n",
       " 'the',\n",
       " 'world',\n",
       " 'we',\n",
       " 'all',\n",
       " 'know',\n",
       " '.',\n",
       " 'Stephen',\n",
       " 'Hawking',\n",
       " 'made',\n",
       " 'it',\n",
       " 'no',\n",
       " 'secret',\n",
       " 'of',\n",
       " 'his',\n",
       " 'concern',\n",
       " 'about',\n",
       " 'the',\n",
       " 'rise',\n",
       " 'of',\n",
       " 'superhuman',\n",
       " 'AI',\n",
       " 'that',\n",
       " 'eventually',\n",
       " 'would',\n",
       " 'escape',\n",
       " 'earth',\n",
       " 'to',\n",
       " 'a',\n",
       " 'new',\n",
       " 'planet',\n",
       " '.',\n",
       " 'No',\n",
       " 'this',\n",
       " 'isn',\n",
       " 't',\n",
       " 'a',\n",
       " 'plot',\n",
       " 'of',\n",
       " 'Black',\n",
       " 'Mirror',\n",
       " '.',\n",
       " 'Right',\n",
       " 'now',\n",
       " 'Superhumans',\n",
       " 'may',\n",
       " 'not',\n",
       " 'be',\n",
       " 'a',\n",
       " 'reality',\n",
       " 'but',\n",
       " 'AI',\n",
       " 'is',\n",
       " '.',\n",
       " 'Homo',\n",
       " 'Deus',\n",
       " 'the',\n",
       " 'emergence',\n",
       " 'of',\n",
       " 'the',\n",
       " 'new',\n",
       " 'Digital',\n",
       " 'God',\n",
       " 'using',\n",
       " 'AI',\n",
       " '.',\n",
       " 'God',\n",
       " 'must',\n",
       " 'also',\n",
       " 'worry',\n",
       " 'as',\n",
       " 'AI',\n",
       " 'might',\n",
       " 'take',\n",
       " 'his',\n",
       " 'job',\n",
       " '.',\n",
       " 'Here',\n",
       " 's',\n",
       " 'some',\n",
       " 'Career',\n",
       " 'Advice',\n",
       " 'have',\n",
       " 'you',\n",
       " 'thought',\n",
       " 'about',\n",
       " 'being',\n",
       " 'a',\n",
       " 'Robot',\n",
       " 'The',\n",
       " 'fear',\n",
       " 'that',\n",
       " 'AI',\n",
       " 'would',\n",
       " 'automate',\n",
       " 'all',\n",
       " 'jobs',\n",
       " 'in',\n",
       " 'the',\n",
       " 'future',\n",
       " 'eventually',\n",
       " 'leaving',\n",
       " 'all',\n",
       " 'humans',\n",
       " 'jobless',\n",
       " 'has',\n",
       " 'been',\n",
       " 'daunting',\n",
       " 'for',\n",
       " 'many',\n",
       " 'workers',\n",
       " 'today',\n",
       " '.',\n",
       " 'Statistics',\n",
       " 'show',\n",
       " 'that',\n",
       " 'nearly',\n",
       " '37',\n",
       " 'of',\n",
       " 'workers',\n",
       " 'worry',\n",
       " 'about',\n",
       " 'losing',\n",
       " 'their',\n",
       " 'jobs',\n",
       " 'to',\n",
       " 'robots',\n",
       " '.',\n",
       " 'While',\n",
       " 'another',\n",
       " 'thought',\n",
       " 'that',\n",
       " 'many',\n",
       " 'people',\n",
       " 'believe',\n",
       " 'is',\n",
       " 'that',\n",
       " 'though',\n",
       " 'the',\n",
       " 'rise',\n",
       " 'of',\n",
       " 'AI',\n",
       " 'with',\n",
       " 'result',\n",
       " 'in',\n",
       " 'automating',\n",
       " 'most',\n",
       " 'of',\n",
       " 'the',\n",
       " 'jobs',\n",
       " 'in',\n",
       " 'the',\n",
       " 'future',\n",
       " 'however',\n",
       " 'it',\n",
       " 'also',\n",
       " 'will',\n",
       " 'create',\n",
       " 'millions',\n",
       " 'of',\n",
       " 'new',\n",
       " 'job',\n",
       " 'opportunities',\n",
       " '.',\n",
       " 'AI',\n",
       " 'is',\n",
       " 'already',\n",
       " 'replacing',\n",
       " 'most',\n",
       " 'manual',\n",
       " 'and',\n",
       " 'repetitive',\n",
       " 'tasks',\n",
       " '.',\n",
       " 'For',\n",
       " 'example',\n",
       " 'buying',\n",
       " 'a',\n",
       " 'metro',\n",
       " 'ticket',\n",
       " 'or',\n",
       " 'a',\n",
       " 'movie',\n",
       " 'ticket',\n",
       " 'is',\n",
       " 'now',\n",
       " 'almost',\n",
       " 'a',\n",
       " 'human',\n",
       " 'less',\n",
       " 'interaction',\n",
       " '.',\n",
       " 'Each',\n",
       " 'year',\n",
       " 'the',\n",
       " 'number',\n",
       " 'of',\n",
       " 'industrial',\n",
       " 'robot',\n",
       " 'jobs',\n",
       " 'increases',\n",
       " 'by',\n",
       " '14',\n",
       " '.',\n",
       " 'At',\n",
       " 'this',\n",
       " 'rate',\n",
       " 'it',\n",
       " 's',\n",
       " 'predicted',\n",
       " 'that',\n",
       " 'the',\n",
       " '20',\n",
       " 'million',\n",
       " 'jobs',\n",
       " 'in',\n",
       " 'the',\n",
       " 'manufacturing',\n",
       " 'industry',\n",
       " 'will',\n",
       " 'be',\n",
       " 'replaced',\n",
       " 'by',\n",
       " 'robots',\n",
       " 'due',\n",
       " 'to',\n",
       " 'automation',\n",
       " '.',\n",
       " 'The',\n",
       " 'coronavirus',\n",
       " 'pandemic',\n",
       " 'and',\n",
       " 'recession',\n",
       " 'have',\n",
       " 'boosted',\n",
       " 'the',\n",
       " 'demand',\n",
       " 'for',\n",
       " 'automation',\n",
       " '.',\n",
       " 'The',\n",
       " 'Robotic',\n",
       " 'Process',\n",
       " 'Automation',\n",
       " 'RPA',\n",
       " 'Software',\n",
       " 'industry',\n",
       " 'has',\n",
       " 'experienced',\n",
       " 'an',\n",
       " 'increase',\n",
       " 'of',\n",
       " '19.53',\n",
       " 'in',\n",
       " 'the',\n",
       " 'year',\n",
       " '2021',\n",
       " '.',\n",
       " 'Coronavirus',\n",
       " 'pandemic',\n",
       " 'has',\n",
       " 'increased',\n",
       " 'interest',\n",
       " 'in',\n",
       " 'technology',\n",
       " 'that',\n",
       " 'reduces',\n",
       " 'human',\n",
       " 'contact',\n",
       " 'as',\n",
       " 'minimal',\n",
       " 'for',\n",
       " 'making',\n",
       " 'workplaces',\n",
       " 'safe',\n",
       " '.',\n",
       " 'Our',\n",
       " 'workplaces',\n",
       " 'will',\n",
       " 'look',\n",
       " 'much',\n",
       " 'different',\n",
       " 'in',\n",
       " 'the',\n",
       " 'next',\n",
       " 'five',\n",
       " 'to',\n",
       " 'ten',\n",
       " 'years',\n",
       " '.',\n",
       " 'AI',\n",
       " 'will',\n",
       " 'help',\n",
       " 'humans',\n",
       " 'in',\n",
       " 'simplifying',\n",
       " 'repetitive',\n",
       " 'processes',\n",
       " '.',\n",
       " 'The',\n",
       " 'two',\n",
       " 'most',\n",
       " 'important',\n",
       " 'catalysts',\n",
       " 'for',\n",
       " 'the',\n",
       " 'future',\n",
       " 'of',\n",
       " 'work',\n",
       " 'are',\n",
       " 'the',\n",
       " 'two',\n",
       " 'D',\n",
       " 's',\n",
       " 'Digitization',\n",
       " 'and',\n",
       " 'Datafication',\n",
       " '.',\n",
       " 'Digitalization',\n",
       " 'is',\n",
       " 'converting',\n",
       " 'data',\n",
       " 'to',\n",
       " 'digital',\n",
       " 'formats',\n",
       " 'computer',\n",
       " 'readable',\n",
       " '.',\n",
       " 'For',\n",
       " 'example',\n",
       " 'text',\n",
       " 'to',\n",
       " 'Html',\n",
       " 'analog',\n",
       " 'video',\n",
       " 'to',\n",
       " 'YouTube',\n",
       " 'video',\n",
       " '.',\n",
       " 'Digitization',\n",
       " 'helps',\n",
       " 'in',\n",
       " 'increasing',\n",
       " 'data',\n",
       " 'exponentially',\n",
       " '.',\n",
       " 'Datafication',\n",
       " 'is',\n",
       " 'quantifying',\n",
       " 'human',\n",
       " 'life',\n",
       " 'to',\n",
       " 'data',\n",
       " 'and',\n",
       " 'improving',\n",
       " 'the',\n",
       " 'data',\n",
       " 'driven',\n",
       " 'business',\n",
       " 'model',\n",
       " '.',\n",
       " 'By',\n",
       " '2025',\n",
       " 'it',\n",
       " 'is',\n",
       " 'forecasted',\n",
       " 'that',\n",
       " 'the',\n",
       " 'digital',\n",
       " 'transformation',\n",
       " 'space',\n",
       " 'will',\n",
       " 'build',\n",
       " 'in',\n",
       " 'a',\n",
       " '$',\n",
       " '3',\n",
       " '294',\n",
       " 'billion',\n",
       " 'industry',\n",
       " '!',\n",
       " 'One',\n",
       " 'thing',\n",
       " 'is',\n",
       " 'clear',\n",
       " 'no',\n",
       " 'data',\n",
       " 'no',\n",
       " 'future',\n",
       " 'of',\n",
       " 'work',\n",
       " '.',\n",
       " 'What',\n",
       " 'we',\n",
       " 'find',\n",
       " 'is',\n",
       " 'that',\n",
       " 'the',\n",
       " 'future',\n",
       " 'of',\n",
       " 'data',\n",
       " 'and',\n",
       " 'the',\n",
       " 'future',\n",
       " 'of',\n",
       " 'work',\n",
       " 'will',\n",
       " 'go',\n",
       " 'hand',\n",
       " 'in',\n",
       " 'hand',\n",
       " '.',\n",
       " 'The',\n",
       " 'total',\n",
       " 'volume',\n",
       " 'of',\n",
       " 'data',\n",
       " 'in',\n",
       " 'the',\n",
       " 'datasphere',\n",
       " 'that',\n",
       " 'is',\n",
       " 'created',\n",
       " 'captured',\n",
       " 'copied',\n",
       " 'and',\n",
       " 'consumed',\n",
       " 'in',\n",
       " 'the',\n",
       " 'world',\n",
       " 'is',\n",
       " 'predicted',\n",
       " 'to',\n",
       " 'reach',\n",
       " '175',\n",
       " 'zettabytes',\n",
       " 'by',\n",
       " '2025',\n",
       " '.',\n",
       " 'To',\n",
       " 'give',\n",
       " 'you',\n",
       " 'a',\n",
       " 'much',\n",
       " 'better',\n",
       " 'picture',\n",
       " 'for',\n",
       " 'understanding',\n",
       " 'if',\n",
       " 'we',\n",
       " 'represent',\n",
       " 'the',\n",
       " 'digital',\n",
       " 'universe',\n",
       " 'as',\n",
       " 'stacks',\n",
       " 'of',\n",
       " 'tablets',\n",
       " 'there',\n",
       " 'would',\n",
       " 'be',\n",
       " '27.25',\n",
       " 'stacks',\n",
       " 'from',\n",
       " 'earth',\n",
       " 'to',\n",
       " 'the',\n",
       " 'moon',\n",
       " '.',\n",
       " 'It',\n",
       " 's',\n",
       " 'time',\n",
       " 'to',\n",
       " 'prepare',\n",
       " 'for',\n",
       " 'the',\n",
       " 'data',\n",
       " 'dominated',\n",
       " 'future',\n",
       " 'as',\n",
       " 'Industry',\n",
       " '4.0/Fourth',\n",
       " 'Industrial',\n",
       " 'Revolution',\n",
       " 'has',\n",
       " 'begun',\n",
       " '.',\n",
       " 'So',\n",
       " 'let',\n",
       " 's',\n",
       " 'see',\n",
       " 'how',\n",
       " 'artificial',\n",
       " 'intelligence',\n",
       " 'will',\n",
       " 'affect',\n",
       " 'the',\n",
       " 'following',\n",
       " 'fields',\n",
       " ':',\n",
       " 'Human',\n",
       " 'Resource',\n",
       " ':',\n",
       " 'Nowadays',\n",
       " 'recruiters',\n",
       " 'use',\n",
       " 'AI',\n",
       " 'powered',\n",
       " 'tools',\n",
       " 'for',\n",
       " 'hiring',\n",
       " 'workers',\n",
       " '.',\n",
       " 'Using',\n",
       " 'these',\n",
       " 'tools',\n",
       " 'recruiters',\n",
       " 'get',\n",
       " 'insights',\n",
       " 'into',\n",
       " 'a',\n",
       " 'candidate',\n",
       " 's',\n",
       " 'skills',\n",
       " 'personality',\n",
       " 'and',\n",
       " 'even',\n",
       " 'check',\n",
       " 'whether',\n",
       " 'the',\n",
       " 'candidate',\n",
       " 'is',\n",
       " 'fit',\n",
       " 'for',\n",
       " 'the',\n",
       " 'organization',\n",
       " '.',\n",
       " 'For',\n",
       " 'example',\n",
       " 'the',\n",
       " 'company',\n",
       " 'AllyO',\n",
       " 'first',\n",
       " 'identifies',\n",
       " 'high',\n",
       " 'potential',\n",
       " 'candidates',\n",
       " 'through',\n",
       " 'assessment',\n",
       " 'and',\n",
       " 'smart',\n",
       " 'screening',\n",
       " 'and',\n",
       " 'then',\n",
       " 'automatically',\n",
       " 'schedules',\n",
       " 'interviews',\n",
       " 'using',\n",
       " 'AI',\n",
       " '.',\n",
       " 'HR',\n",
       " 'departments',\n",
       " 'at',\n",
       " 'large',\n",
       " 'companies',\n",
       " 'receive',\n",
       " 'hundreds',\n",
       " 'of',\n",
       " 'resumes',\n",
       " 'for',\n",
       " 'a',\n",
       " 'job',\n",
       " 'opening',\n",
       " '.',\n",
       " 'Entry',\n",
       " 'level',\n",
       " 'roles',\n",
       " 'focusing',\n",
       " 'on',\n",
       " 'screening',\n",
       " 'and',\n",
       " 'scheduling',\n",
       " 'will',\n",
       " 'be',\n",
       " 'automated',\n",
       " '.',\n",
       " 'AI',\n",
       " 'will',\n",
       " 'automate',\n",
       " 'specific',\n",
       " 'HR',\n",
       " 'jobs',\n",
       " 'not',\n",
       " 'HR',\n",
       " 'roles',\n",
       " '.',\n",
       " 'A',\n",
       " 'Deloitte',\n",
       " 'study',\n",
       " 'found',\n",
       " 'that',\n",
       " 'AI',\n",
       " 'has',\n",
       " 'already',\n",
       " 'eliminated',\n",
       " '800',\n",
       " '000',\n",
       " 'low',\n",
       " 'skilled',\n",
       " 'jobs',\n",
       " 'in',\n",
       " 'the',\n",
       " 'UK',\n",
       " 'but',\n",
       " '3.5',\n",
       " 'million',\n",
       " 'new',\n",
       " 'jobs',\n",
       " 'were',\n",
       " 'also',\n",
       " 'created',\n",
       " '.',\n",
       " 'Roles',\n",
       " 'that',\n",
       " 'focus',\n",
       " 'on',\n",
       " 'complex',\n",
       " 'decisions',\n",
       " 'like',\n",
       " 'resolving',\n",
       " 'disputes',\n",
       " 'within',\n",
       " 'a',\n",
       " 'department',\n",
       " 'will',\n",
       " 'continue',\n",
       " 'to',\n",
       " 'be',\n",
       " 'a',\n",
       " 'very',\n",
       " 'human',\n",
       " 'endeavor',\n",
       " '.',\n",
       " 'Finance',\n",
       " 'and',\n",
       " 'Accounting',\n",
       " ':',\n",
       " 'In',\n",
       " '2015',\n",
       " 'a',\n",
       " 'report',\n",
       " 'from',\n",
       " 'Accenture',\n",
       " 'named',\n",
       " 'Finance',\n",
       " '2020',\n",
       " ':',\n",
       " 'death',\n",
       " 'by',\n",
       " 'digital',\n",
       " 'predicted',\n",
       " 'that',\n",
       " '40',\n",
       " 'percent',\n",
       " 'of',\n",
       " 'transactional',\n",
       " 'accounting',\n",
       " 'work',\n",
       " 'would',\n",
       " 'be',\n",
       " 'automated',\n",
       " 'by',\n",
       " '2020',\n",
       " '.',\n",
       " 'Has',\n",
       " 'technology',\n",
       " 'replaced',\n",
       " 'the',\n",
       " 'human',\n",
       " 'factor',\n",
       " 'Well',\n",
       " 'AI',\n",
       " 'has',\n",
       " 'created',\n",
       " 'new',\n",
       " 'jobs',\n",
       " 'involving',\n",
       " 'managing',\n",
       " 'the',\n",
       " 'AI',\n",
       " 'system',\n",
       " 'and',\n",
       " 'using',\n",
       " 'the',\n",
       " 'information',\n",
       " 'to',\n",
       " 'create',\n",
       " 'insights',\n",
       " '.',\n",
       " 'For',\n",
       " 'example',\n",
       " 'accounting',\n",
       " 'software',\n",
       " 'has',\n",
       " 'already',\n",
       " 'automated',\n",
       " 'bookkeeping',\n",
       " 'tasks',\n",
       " 'that',\n",
       " 'used',\n",
       " 'to',\n",
       " 'be',\n",
       " 'done',\n",
       " 'by',\n",
       " 'humans',\n",
       " 'but',\n",
       " 'that',\n",
       " 's',\n",
       " 'only',\n",
       " 'opened',\n",
       " 'the',\n",
       " 'door',\n",
       " 'for',\n",
       " 'former',\n",
       " 'bookkeepers',\n",
       " 'to',\n",
       " 'learn',\n",
       " 'skills',\n",
       " 'needed',\n",
       " 'to',\n",
       " 'run',\n",
       " 'and',\n",
       " 'manage',\n",
       " 'the',\n",
       " 'software',\n",
       " 'for',\n",
       " 'employers',\n",
       " 'and',\n",
       " 'clients',\n",
       " '.',\n",
       " 'Advisors',\n",
       " 'are',\n",
       " 'another',\n",
       " 'crucial',\n",
       " 'role',\n",
       " ...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = word_tokenize(texts)\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10016a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"D:\\lab\\Dataset\\Stopwords_Blackcoffer.txt\", 'r') as file:\n",
    "    data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c567cbef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ERNST\\nYOUNG\\nDELOITTE\\nTOUCHE\\nKPMG\\nPRICEWATERHOUSECOOPERS\\nPRICEWATERHOUSE\\nCOOPERS\\nAFGHANI\\nARIARY\\nBAHT\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d65bfa92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ERNST,YOUNG,DELOITTE,TOUCHE,KPMG,PRICEWATERHOUSECOOPERS,PRICEWATERHOUSE,COOPERS,AFGHANI,ARIARY,BAHT,'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.replace('\\n', ',')\n",
    "data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ef23874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It year 2060 . An automaton Research Laboratory Scientist Warning ! Error Occurred Reformatting Hard Disk Now ! The scientist panics . Automaton Ha ! Ha ! Just Kidding ! . Funny Right . Before joke re'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_texts = \" \".join([i for i in token if i not in data])\n",
    "new_texts[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea392da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length before cleaning:  7452\n",
      "length after cleaning:  4973\n"
     ]
    }
   ],
   "source": [
    "print(\"length before cleaning: \", len(texts))\n",
    "print(\"length after cleaning: \", len(new_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f456147",
   "metadata": {},
   "source": [
    "# Negative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc3618a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"D:\\\\lab\\\\Dataset\\\\Negative_words.txt\", 'r') as file:\n",
    "    negative = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2123c4cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2-faced\\n2-faces\\nabnormal\\nabolish\\nabominable\\nabominably\\nabominate\\nabomination\\nabort\\naborted\\naborts\\nabrade\\nabrasive\\nabrupt\\nabruptly\\nabscond\\nabsence\\nabsent-minded\\nabsentee\\nabsurd\\nabsurdity\\nabsurdly\\nabsurdness\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative[:206]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28f44c97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2-faced,2-faces,abnormal,abolish,abominable,abominably,abominate,abomination,abort,aborted,aborts,abrade,abrasive,abrupt,abruptly,abscond,absence,absent-minded,absentee,absurd,absurdity,absurdly,absurdness,'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative = negative.replace('\\n', ',')\n",
    "negative[:206]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31a272c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['joke', 'realistic', 'joke', 'joke', 'show', 'thought', 'joke', 'giving', 'human', 'decade', 'work', 'question', 'power', 'found', 'decision', 'thought', 'question', 'human', 'friend', 'foe', 'fiction', 'question', 'solve', 'problems', 'destruction', 'secret', 'earth', 'plot', 'worry', 'job', 'thought', 'fear', 'jobless', 'daunting', 'show', 'worry', 'losing', 'thought', 'job', 'repetitive', 'human', 'rate', 'due', 'recession', 'experienced', 'interest', 'human', 'safe', 'repetitive', 'important', 'work', 'readable', 'human', 'life', 'work', 'work', 'total', 'reach', 'understanding', 'represent', 'earth', 'prepare', 'affect', 'powered', 'check', 'fit', 'high', 'job', 'found', 'skilled', 'complex', 'continue', 'human', 'death', 'work', 'human', 'factor', 'needed', 'run', 'manage', 'proper', 'draw', 'understand', 'effective', 'eye', 'ago', 'obsolete', 'relevant', 'work', 'work', 'important', 'led', 'losing', 'make', 'humane', 'disrupt']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_texts = word_tokenize(new_texts)\n",
    "negative_words = [i for i in new_texts if i in negative]\n",
    "print(negative_words)\n",
    "len(negative_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b24f011",
   "metadata": {},
   "source": [
    "# Positive words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b003620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a+\\nabound\\nabounds\\nabundance\\nabundant\\naccessable\\naccessible\\nacclaim\\nacclaimed\\nacclamation\\naccolade\\naccolades\\naccommodative\\naccomodative\\naccomplish\\naccomplished\\naccomplishment\\naccomplishments\\naccurate\\naccurately\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"D:\\\\lab\\\\Dataset\\\\Positive_words.txt\", 'r') as file:\n",
    "    positive = file.read()\n",
    "positive[:210]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66ed38c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a+,abound,abounds,abundance,abundant,accessable,accessible,acclaim,acclaimed,acclamation,accolade,accolades,accommodative,accomodative,accomplish,accomplished,accomplishment,accomplishments,accurate,accurately,'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive = positive.replace('\\n',',')\n",
    "positive[:210]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc1387e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['realistic', 'made', 'thought', 'idea', 'human', 'enjoy', 'future', 'work', 'simple', 'question', 'power', 'found', 'thought', 'question', 'human', 'friend', 'question', 'solve', 'world', 'made', 'thought', 'fear', 'future', 'thought', 'future', 'human', 'rate', 'interest', 'human', 'safe', 'simplifying', 'important', 'future', 'work', 'readable', 'human', 'life', 'improving', 'future', 'work', 'future', 'future', 'work', 'world', 'reach', 'picture', 'dominated', 'future', 'intelligence', 'affect', 'fit', 'high', 'smart', 'large', 'receive', 'found', 'skilled', 'human', 'work', 'human', 'factor', 'run', 'manage', 'gained', 'improve', 'helped', 'proper', 'helped', 'scale', 'excel', 'understand', 'effective', 'eye', 'work', 'work', 'important', 'future', 'revolution', 'led', 'ended', 'optimistic', 'picture', 'future', 'future', 'replace', 'humane', 'boost']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_words = [i for i in new_texts if i in positive]\n",
    "print(positive_words)\n",
    "len(positive_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b59646d",
   "metadata": {},
   "source": [
    "#### positive Score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d0fbab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87\n"
     ]
    }
   ],
   "source": [
    "pos_score = len(positive_words)\n",
    "print(pos_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a6cc12",
   "metadata": {},
   "source": [
    "#### Negative Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df0766cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96\n"
     ]
    }
   ],
   "source": [
    "neg_score=len(negative_words)\n",
    "print(neg_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84da7ab7",
   "metadata": {},
   "source": [
    "#### Polarity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21e88c2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.05"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Polarity_Score = (pos_score - neg_score)/((pos_score + neg_score) + 0.000001)\n",
    "round(Polarity_Score,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08bcb4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words after cleaning : 713\n"
     ]
    }
   ],
   "source": [
    "word_count = len(new_texts)\n",
    "print(\"number of words after cleaning :\",word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16495ba1",
   "metadata": {},
   "source": [
    "#### Subjectivity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "841713a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Subjectivity_Score = (pos_score + neg_score)/ ((word_count) + 0.000001)\n",
    "round(Subjectivity_Score,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7c370e",
   "metadata": {},
   "source": [
    "# Analysis of Readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "10209a7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1315"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "word_tokens = nltk.word_tokenize(texts)\n",
    "No_of_words = len(word_tokens)\n",
    "No_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bfac2ab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokens = nltk.sent_tokenize(texts)\n",
    "No_of_sents = len(sent_tokens)\n",
    "No_of_sents "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1576acfe",
   "metadata": {},
   "source": [
    "### Average sentence Length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "11d6a7ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.53"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Avg_Sents_Length = No_of_words / No_of_sents\n",
    "round(Avg_Sents_Length,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc20ee3d",
   "metadata": {},
   "source": [
    "### Percentage of Complex words\n",
    "Complex words: words with more than 2 syllable are called complex words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eba6a8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of complex words: 178\n",
      "Total number of words: 7452\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import cmudict\n",
    "cmud = cmudict.dict()\n",
    "\n",
    "# Defining a function to count syllables in a word\n",
    "def count_syllables(word):\n",
    "    phonemes = cmud[word.lower()][0] \n",
    "    return len([s for s in phonemes if s[-1].isdigit()])\n",
    "\n",
    "# Identifing complex words\n",
    "w_tokens = [i for i in word_tokens if i in cmud]\n",
    "complex_words = [word for word in w_tokens if count_syllables(word) > 2]\n",
    "\n",
    "# Calculatingnumber of complex words\n",
    "num_complex_words = len(complex_words)\n",
    "\n",
    "print(\"Number of complex words:\", num_complex_words)\n",
    "print(\"Total number of words:\", len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e982c0a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41.87"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Perc_of_Complex_words = len(texts) / num_complex_words\n",
    "round(Perc_of_Complex_words,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e948d1",
   "metadata": {},
   "source": [
    "### Fog Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "95fb10bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sentence length: 75.0\n"
     ]
    }
   ],
   "source": [
    "# Tokenize each sentence into words\n",
    "words = [word_tokens for sentence in sent_tokens]\n",
    "\n",
    "# Calculate the average sentence length\n",
    "avg_sent_len = sum(No_of_sents for sentence in words) / No_of_sents\n",
    "\n",
    "print(\"Average sentence length:\", avg_sent_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "38f5a93b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46.75"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Fog_index = 0.4 * (avg_sent_len + Perc_of_Complex_words)\n",
    "round(Fog_index,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4aacea",
   "metadata": {},
   "source": [
    "# Average Number of Words Per Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "45da1407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.53"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_no_of_words_per_sent = No_of_words / No_of_sents\n",
    "round(avg_no_of_words_per_sent,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d9b493",
   "metadata": {},
   "source": [
    "# Complex Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "75304424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complex word count: 178\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import cmudict\n",
    "cmud = cmudict.dict()\n",
    "\n",
    "# Defining a function to count syllables in a word\n",
    "def count_syllables(word):\n",
    "    phonemes = cmud[word.lower()][0] \n",
    "    return len([s for s in phonemes if s[-1].isdigit()])\n",
    "\n",
    "# Identifing complex words\n",
    "w_tokens = [i for i in word_tokens if i in cmud]\n",
    "complex_words = [word for word in w_tokens if count_syllables(word) > 2]\n",
    "\n",
    "# Calculatingnumber of complex words\n",
    "num_complex_words = len(complex_words)\n",
    "\n",
    "print(\"complex word count:\", num_complex_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ea5188",
   "metadata": {},
   "source": [
    "# Word Count\n",
    "number of words after cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b4fb0c72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "713"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8076ff",
   "metadata": {},
   "source": [
    "# Syllable Count Per Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bc568045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a word to get syllable: statistics\n",
      "Number of syllable in a word : 3\n"
     ]
    }
   ],
   "source": [
    "word = input(\"Enter a word to get syllable: \")\n",
    "def count_syllables(word):\n",
    "    phonemes = cmud[word.lower()][0] \n",
    "    return len([s for s in phonemes if s[-1].isdigit()])\n",
    "\n",
    "print(\"Number of syllable in a word :\",count_syllables(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3a18f6",
   "metadata": {},
   "source": [
    "# Personal Pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0b8e0d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Personal pronoun frequency: {'it': 7, 'you': 5, 'i': 1, 'us': 2, 'we': 3}\n",
      "total number of pronouns in a article : 18\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Define a regex pattern to match personal pronouns\n",
    "pattern = r'\\b(I|you|he|she|it|we|they|me|him|her|us|them)\\b'\n",
    "\n",
    "# Count the frequency of personal pronouns\n",
    "pronoun_freq = {}\n",
    "for pronoun in re.findall(pattern, texts, re.IGNORECASE):\n",
    "    pronoun = pronoun.lower()\n",
    "    if pronoun in pronoun_freq:\n",
    "        pronoun_freq[pronoun] += 1\n",
    "    else:\n",
    "        pronoun_freq[pronoun] = 1\n",
    "\n",
    "print(\"Personal pronoun frequency:\", pronoun_freq)\n",
    "def returnSum(dict):\n",
    " \n",
    "    sum = 0\n",
    "    for i in pronoun_freq.values():\n",
    "        sum = sum + i\n",
    " \n",
    "    return sum\n",
    "print(\"total number of pronouns in a article :\",returnSum(pronoun_freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fc87f8",
   "metadata": {},
   "source": [
    "# Average Word Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5bde37d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average word length: 4.67\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total number of characters in all words\n",
    "total_chars = sum(len(word) for word in word_tokens)\n",
    "\n",
    "avg_word_length = total_chars / No_of_words\n",
    "\n",
    "print(\"Average word length:\", round(avg_word_length,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dc25ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819012c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
