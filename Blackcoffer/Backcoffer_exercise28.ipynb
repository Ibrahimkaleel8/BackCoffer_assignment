{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9312f26",
   "metadata": {},
   "source": [
    "# Data scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef4785ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac2f2459",
   "metadata": {},
   "outputs": [],
   "source": [
    "req = requests.get(\"https://insights.blackcoffer.com/impact-of-ai-in-health-and-medicine/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45f9fa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(req.content, \"html.parser\")\n",
    "res = soup.title\n",
    "paras = soup.find_all('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbb256b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI allows those in training to go through naturalistic simulations in a way that simple computer-driven algorithms cannot. The advent of natural speech and the ability of an AI  computer to draw instantly on a large database of scenarios means the response to questions, decisions, or advice from a trainee can challenge in a way that humans cannot in health and medicine Wearable health trackers-like those from FitBit, Apple, Garmin, and others- monitor health rate and activity levels. They can send alerts to the user to get more exercise and can share this information with doctors. Machine learning algorithms can process unimaginable amounts of information in the blink of an eye and provide more precision than humans in spotting even the smallest detail in medical imaging.  A few of them are Blackford, Zebra, Enlitic, Lunit. The company “Zebra Medical Vision” developed a new platform called profound, which analyzes all types of medical imaging reports that are able to find every sign of potential conditions such as osteoporosis, aortic aneurysms, and many more with a 90% accuracy rate. For eg, the digital health firm #HealthTap developed “Dr. AI”, and apps like Babylon in the UK use AI to give medical consultations based on personal medical history and common medical knowledge. Users report their symptoms into the app, which uses speech recognition to compare against a database of illness and asks patients to specify symptoms to triage whether they should go to the ED, urgent care, or a primary care doctor. AI robot-assisted surgery: Robots have been used in medicine for more than 30 years. Surgical robots can either aid a human surgeon or execute operations by themselves. They’re also used in hospitals and labs for repetitive tasks, in rehabilitation, physical therapy, and in support of those with long-term conditions. Health chatbots such as Babylon, Ada, and mostly close-ended communications. #MICROSOFT: Predictive analysis in vision care #GOOGLE: clinical decision support in breast cancer diagnosis #IBM WATSON: precision medicine in population health management Actionable medical insights:  An ever-increasing amount of medical data are being digitized at all public and private healthcare institutions. However, by its very nature, this kind of data is messy and unstructured. Unlike other types of business data, where traditional statistical methods can be used for quick insights, patient data is not particularly amenable to simple modeling and analytics tools. For eg.- Enlitic, a San Francisco-based start-up, has a mission of mixing intelligence with empathy and leverage the power of AI in health and medicine for precisely generating. Therefore, a massive parallel effort to rationalize the legal and policy-making is needed to bring the full benefit of advancement in AI technologies into the healthcare space. As technologies and AI/ML enthusiasts, we can only hope for such a bright future where the power of this intelligence. \n"
     ]
    }
   ],
   "source": [
    "texts = \" \".join([paragraph.text.strip() for paragraph in paras])\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e79f5f60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2981"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8f1fcc",
   "metadata": {},
   "source": [
    "Removing Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5285c8a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2981"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "texts = re.sub(r'[\\'\\“\\”\\()\\%\\,\\-\\'\\’\\?\\ ]', ' ', texts)\n",
    "texts[0:200]\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fafe7be",
   "metadata": {},
   "source": [
    "# Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "089b8663",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "280876b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AI',\n",
       " 'allows',\n",
       " 'those',\n",
       " 'in',\n",
       " 'training',\n",
       " 'to',\n",
       " 'go',\n",
       " 'through',\n",
       " 'naturalistic',\n",
       " 'simulations',\n",
       " 'in',\n",
       " 'a',\n",
       " 'way',\n",
       " 'that',\n",
       " 'simple',\n",
       " 'computer',\n",
       " 'driven',\n",
       " 'algorithms',\n",
       " 'can',\n",
       " 'not',\n",
       " '.',\n",
       " 'The',\n",
       " 'advent',\n",
       " 'of',\n",
       " 'natural',\n",
       " 'speech',\n",
       " 'and',\n",
       " 'the',\n",
       " 'ability',\n",
       " 'of',\n",
       " 'an',\n",
       " 'AI',\n",
       " 'computer',\n",
       " 'to',\n",
       " 'draw',\n",
       " 'instantly',\n",
       " 'on',\n",
       " 'a',\n",
       " 'large',\n",
       " 'database',\n",
       " 'of',\n",
       " 'scenarios',\n",
       " 'means',\n",
       " 'the',\n",
       " 'response',\n",
       " 'to',\n",
       " 'questions',\n",
       " 'decisions',\n",
       " 'or',\n",
       " 'advice',\n",
       " 'from',\n",
       " 'a',\n",
       " 'trainee',\n",
       " 'can',\n",
       " 'challenge',\n",
       " 'in',\n",
       " 'a',\n",
       " 'way',\n",
       " 'that',\n",
       " 'humans',\n",
       " 'can',\n",
       " 'not',\n",
       " 'in',\n",
       " 'health',\n",
       " 'and',\n",
       " 'medicine',\n",
       " 'Wearable',\n",
       " 'health',\n",
       " 'trackers',\n",
       " 'like',\n",
       " 'those',\n",
       " 'from',\n",
       " 'FitBit',\n",
       " 'Apple',\n",
       " 'Garmin',\n",
       " 'and',\n",
       " 'others',\n",
       " 'monitor',\n",
       " 'health',\n",
       " 'rate',\n",
       " 'and',\n",
       " 'activity',\n",
       " 'levels',\n",
       " '.',\n",
       " 'They',\n",
       " 'can',\n",
       " 'send',\n",
       " 'alerts',\n",
       " 'to',\n",
       " 'the',\n",
       " 'user',\n",
       " 'to',\n",
       " 'get',\n",
       " 'more',\n",
       " 'exercise',\n",
       " 'and',\n",
       " 'can',\n",
       " 'share',\n",
       " 'this',\n",
       " 'information',\n",
       " 'with',\n",
       " 'doctors',\n",
       " '.',\n",
       " 'Machine',\n",
       " 'learning',\n",
       " 'algorithms',\n",
       " 'can',\n",
       " 'process',\n",
       " 'unimaginable',\n",
       " 'amounts',\n",
       " 'of',\n",
       " 'information',\n",
       " 'in',\n",
       " 'the',\n",
       " 'blink',\n",
       " 'of',\n",
       " 'an',\n",
       " 'eye',\n",
       " 'and',\n",
       " 'provide',\n",
       " 'more',\n",
       " 'precision',\n",
       " 'than',\n",
       " 'humans',\n",
       " 'in',\n",
       " 'spotting',\n",
       " 'even',\n",
       " 'the',\n",
       " 'smallest',\n",
       " 'detail',\n",
       " 'in',\n",
       " 'medical',\n",
       " 'imaging',\n",
       " '.',\n",
       " 'A',\n",
       " 'few',\n",
       " 'of',\n",
       " 'them',\n",
       " 'are',\n",
       " 'Blackford',\n",
       " 'Zebra',\n",
       " 'Enlitic',\n",
       " 'Lunit',\n",
       " '.',\n",
       " 'The',\n",
       " 'company',\n",
       " 'Zebra',\n",
       " 'Medical',\n",
       " 'Vision',\n",
       " 'developed',\n",
       " 'a',\n",
       " 'new',\n",
       " 'platform',\n",
       " 'called',\n",
       " 'profound',\n",
       " 'which',\n",
       " 'analyzes',\n",
       " 'all',\n",
       " 'types',\n",
       " 'of',\n",
       " 'medical',\n",
       " 'imaging',\n",
       " 'reports',\n",
       " 'that',\n",
       " 'are',\n",
       " 'able',\n",
       " 'to',\n",
       " 'find',\n",
       " 'every',\n",
       " 'sign',\n",
       " 'of',\n",
       " 'potential',\n",
       " 'conditions',\n",
       " 'such',\n",
       " 'as',\n",
       " 'osteoporosis',\n",
       " 'aortic',\n",
       " 'aneurysms',\n",
       " 'and',\n",
       " 'many',\n",
       " 'more',\n",
       " 'with',\n",
       " 'a',\n",
       " '90',\n",
       " 'accuracy',\n",
       " 'rate',\n",
       " '.',\n",
       " 'For',\n",
       " 'eg',\n",
       " 'the',\n",
       " 'digital',\n",
       " 'health',\n",
       " 'firm',\n",
       " '#',\n",
       " 'HealthTap',\n",
       " 'developed',\n",
       " 'Dr.',\n",
       " 'AI',\n",
       " 'and',\n",
       " 'apps',\n",
       " 'like',\n",
       " 'Babylon',\n",
       " 'in',\n",
       " 'the',\n",
       " 'UK',\n",
       " 'use',\n",
       " 'AI',\n",
       " 'to',\n",
       " 'give',\n",
       " 'medical',\n",
       " 'consultations',\n",
       " 'based',\n",
       " 'on',\n",
       " 'personal',\n",
       " 'medical',\n",
       " 'history',\n",
       " 'and',\n",
       " 'common',\n",
       " 'medical',\n",
       " 'knowledge',\n",
       " '.',\n",
       " 'Users',\n",
       " 'report',\n",
       " 'their',\n",
       " 'symptoms',\n",
       " 'into',\n",
       " 'the',\n",
       " 'app',\n",
       " 'which',\n",
       " 'uses',\n",
       " 'speech',\n",
       " 'recognition',\n",
       " 'to',\n",
       " 'compare',\n",
       " 'against',\n",
       " 'a',\n",
       " 'database',\n",
       " 'of',\n",
       " 'illness',\n",
       " 'and',\n",
       " 'asks',\n",
       " 'patients',\n",
       " 'to',\n",
       " 'specify',\n",
       " 'symptoms',\n",
       " 'to',\n",
       " 'triage',\n",
       " 'whether',\n",
       " 'they',\n",
       " 'should',\n",
       " 'go',\n",
       " 'to',\n",
       " 'the',\n",
       " 'ED',\n",
       " 'urgent',\n",
       " 'care',\n",
       " 'or',\n",
       " 'a',\n",
       " 'primary',\n",
       " 'care',\n",
       " 'doctor',\n",
       " '.',\n",
       " 'AI',\n",
       " 'robot',\n",
       " 'assisted',\n",
       " 'surgery',\n",
       " ':',\n",
       " 'Robots',\n",
       " 'have',\n",
       " 'been',\n",
       " 'used',\n",
       " 'in',\n",
       " 'medicine',\n",
       " 'for',\n",
       " 'more',\n",
       " 'than',\n",
       " '30',\n",
       " 'years',\n",
       " '.',\n",
       " 'Surgical',\n",
       " 'robots',\n",
       " 'can',\n",
       " 'either',\n",
       " 'aid',\n",
       " 'a',\n",
       " 'human',\n",
       " 'surgeon',\n",
       " 'or',\n",
       " 'execute',\n",
       " 'operations',\n",
       " 'by',\n",
       " 'themselves',\n",
       " '.',\n",
       " 'They',\n",
       " 're',\n",
       " 'also',\n",
       " 'used',\n",
       " 'in',\n",
       " 'hospitals',\n",
       " 'and',\n",
       " 'labs',\n",
       " 'for',\n",
       " 'repetitive',\n",
       " 'tasks',\n",
       " 'in',\n",
       " 'rehabilitation',\n",
       " 'physical',\n",
       " 'therapy',\n",
       " 'and',\n",
       " 'in',\n",
       " 'support',\n",
       " 'of',\n",
       " 'those',\n",
       " 'with',\n",
       " 'long',\n",
       " 'term',\n",
       " 'conditions',\n",
       " '.',\n",
       " 'Health',\n",
       " 'chatbots',\n",
       " 'such',\n",
       " 'as',\n",
       " 'Babylon',\n",
       " 'Ada',\n",
       " 'and',\n",
       " 'mostly',\n",
       " 'close',\n",
       " 'ended',\n",
       " 'communications',\n",
       " '.',\n",
       " '#',\n",
       " 'MICROSOFT',\n",
       " ':',\n",
       " 'Predictive',\n",
       " 'analysis',\n",
       " 'in',\n",
       " 'vision',\n",
       " 'care',\n",
       " '#',\n",
       " 'GOOGLE',\n",
       " ':',\n",
       " 'clinical',\n",
       " 'decision',\n",
       " 'support',\n",
       " 'in',\n",
       " 'breast',\n",
       " 'cancer',\n",
       " 'diagnosis',\n",
       " '#',\n",
       " 'IBM',\n",
       " 'WATSON',\n",
       " ':',\n",
       " 'precision',\n",
       " 'medicine',\n",
       " 'in',\n",
       " 'population',\n",
       " 'health',\n",
       " 'management',\n",
       " 'Actionable',\n",
       " 'medical',\n",
       " 'insights',\n",
       " ':',\n",
       " 'An',\n",
       " 'ever',\n",
       " 'increasing',\n",
       " 'amount',\n",
       " 'of',\n",
       " 'medical',\n",
       " 'data',\n",
       " 'are',\n",
       " 'being',\n",
       " 'digitized',\n",
       " 'at',\n",
       " 'all',\n",
       " 'public',\n",
       " 'and',\n",
       " 'private',\n",
       " 'healthcare',\n",
       " 'institutions',\n",
       " '.',\n",
       " 'However',\n",
       " 'by',\n",
       " 'its',\n",
       " 'very',\n",
       " 'nature',\n",
       " 'this',\n",
       " 'kind',\n",
       " 'of',\n",
       " 'data',\n",
       " 'is',\n",
       " 'messy',\n",
       " 'and',\n",
       " 'unstructured',\n",
       " '.',\n",
       " 'Unlike',\n",
       " 'other',\n",
       " 'types',\n",
       " 'of',\n",
       " 'business',\n",
       " 'data',\n",
       " 'where',\n",
       " 'traditional',\n",
       " 'statistical',\n",
       " 'methods',\n",
       " 'can',\n",
       " 'be',\n",
       " 'used',\n",
       " 'for',\n",
       " 'quick',\n",
       " 'insights',\n",
       " 'patient',\n",
       " 'data',\n",
       " 'is',\n",
       " 'not',\n",
       " 'particularly',\n",
       " 'amenable',\n",
       " 'to',\n",
       " 'simple',\n",
       " 'modeling',\n",
       " 'and',\n",
       " 'analytics',\n",
       " 'tools',\n",
       " '.',\n",
       " 'For',\n",
       " 'eg',\n",
       " '.',\n",
       " 'Enlitic',\n",
       " 'a',\n",
       " 'San',\n",
       " 'Francisco',\n",
       " 'based',\n",
       " 'start',\n",
       " 'up',\n",
       " 'has',\n",
       " 'a',\n",
       " 'mission',\n",
       " 'of',\n",
       " 'mixing',\n",
       " 'intelligence',\n",
       " 'with',\n",
       " 'empathy',\n",
       " 'and',\n",
       " 'leverage',\n",
       " 'the',\n",
       " 'power',\n",
       " 'of',\n",
       " 'AI',\n",
       " 'in',\n",
       " 'health',\n",
       " 'and',\n",
       " 'medicine',\n",
       " 'for',\n",
       " 'precisely',\n",
       " 'generating',\n",
       " '.',\n",
       " 'Therefore',\n",
       " 'a',\n",
       " 'massive',\n",
       " 'parallel',\n",
       " 'effort',\n",
       " 'to',\n",
       " 'rationalize',\n",
       " 'the',\n",
       " 'legal',\n",
       " 'and',\n",
       " 'policy',\n",
       " 'making',\n",
       " 'is',\n",
       " 'needed',\n",
       " 'to',\n",
       " 'bring',\n",
       " 'the',\n",
       " 'full',\n",
       " 'benefit',\n",
       " 'of',\n",
       " 'advancement',\n",
       " 'in',\n",
       " 'AI',\n",
       " 'technologies',\n",
       " 'into',\n",
       " 'the',\n",
       " 'healthcare',\n",
       " 'space',\n",
       " '.',\n",
       " 'As',\n",
       " 'technologies',\n",
       " 'and',\n",
       " 'AI/ML',\n",
       " 'enthusiasts',\n",
       " 'we',\n",
       " 'can',\n",
       " 'only',\n",
       " 'hope',\n",
       " 'for',\n",
       " 'such',\n",
       " 'a',\n",
       " 'bright',\n",
       " 'future',\n",
       " 'where',\n",
       " 'the',\n",
       " 'power',\n",
       " 'of',\n",
       " 'this',\n",
       " 'intelligence',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = word_tokenize(texts)\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10016a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"D:\\lab\\Dataset\\Stopwords_Blackcoffer.txt\", 'r') as file:\n",
    "    data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c567cbef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ERNST\\nYOUNG\\nDELOITTE\\nTOUCHE\\nKPMG\\nPRICEWATERHOUSECOOPERS\\nPRICEWATERHOUSE\\nCOOPERS\\nAFGHANI\\nARIARY\\nBAHT\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d65bfa92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ERNST,YOUNG,DELOITTE,TOUCHE,KPMG,PRICEWATERHOUSECOOPERS,PRICEWATERHOUSE,COOPERS,AFGHANI,ARIARY,BAHT,'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.replace('\\n', ',')\n",
    "data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ef23874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'training naturalistic simulations simple computer driven algorithms . The advent natural speech ability computer draw instantly large database scenarios means response questions decisions advice train'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_texts = \" \".join([i for i in token if i not in data])\n",
    "new_texts[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea392da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length before cleaning:  2981\n",
      "length after cleaning:  2057\n"
     ]
    }
   ],
   "source": [
    "print(\"length before cleaning: \", len(texts))\n",
    "print(\"length after cleaning: \", len(new_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f456147",
   "metadata": {},
   "source": [
    "# Negative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc3618a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"D:\\\\lab\\\\Dataset\\\\Negative_words.txt\", 'r') as file:\n",
    "    negative = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2123c4cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2-faced\\n2-faces\\nabnormal\\nabolish\\nabominable\\nabominably\\nabominate\\nabomination\\nabort\\naborted\\naborts\\nabrade\\nabrasive\\nabrupt\\nabruptly\\nabscond\\nabsence\\nabsent-minded\\nabsentee\\nabsurd\\nabsurdity\\nabsurdly\\nabsurdness\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative[:206]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28f44c97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2-faced,2-faces,abnormal,abolish,abominable,abominably,abominate,abomination,abort,aborted,aborts,abrade,abrasive,abrupt,abruptly,abscond,absence,absent-minded,absentee,absurd,absurdity,absurdly,absurdness,'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative = negative.replace('\\n', ',')\n",
    "negative[:206]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31a272c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['training', 'natural', 'ability', 'draw', 'health', 'health', 'health', 'rate', 'unimaginable', 'eye', 'precision', 'developed', 'sign', 'accuracy', 'rate', 'health', 'firm', 'developed', 'personal', 'common', 'symptoms', 'illness', 'symptoms', 'urgent', 'care', 'care', 'human', 'repetitive', 'support', 'term', 'care', 'decision', 'support', 'cancer', 'precision', 'health', 'nature', 'kind', 'messy', 'patient', 'start', 'mission', 'power', 'health', 'precisely', 'legal', 'needed', 'power']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_texts = word_tokenize(new_texts)\n",
    "negative_words = [i for i in new_texts if i in negative]\n",
    "print(negative_words)\n",
    "len(negative_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b24f011",
   "metadata": {},
   "source": [
    "# Positive words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b003620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a+\\nabound\\nabounds\\nabundance\\nabundant\\naccessable\\naccessible\\nacclaim\\nacclaimed\\nacclamation\\naccolade\\naccolades\\naccommodative\\naccomodative\\naccomplish\\naccomplished\\naccomplishment\\naccomplishments\\naccurate\\naccurately\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"D:\\\\lab\\\\Dataset\\\\Positive_words.txt\", 'r') as file:\n",
    "    positive = file.read()\n",
    "positive[:210]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66ed38c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a+,abound,abounds,abundance,abundant,accessable,accessible,acclaim,acclaimed,acclamation,accolade,accolades,accommodative,accomodative,accomplish,accomplished,accomplishment,accomplishments,accurate,accurately,'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive = positive.replace('\\n',',')\n",
    "positive[:210]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc1387e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['simple', 'advent', 'ability', 'instantly', 'large', 'health', 'health', 'health', 'rate', 'send', 'user', 'eye', 'profound', 'sign', 'rate', 'health', 'firm', 'personal', 'knowledge', 'care', 'care', 'human', 'support', 'term', 'ended', 'vision', 'care', 'support', 'health', 'amount', 'kind', 'quick', 'patient', 'amenable', 'simple', 'mission', 'intelligence', 'empathy', 'leverage', 'power', 'health', 'precisely', 'parallel', 'effort', 'benefit', 'bright', 'future', 'power', 'intelligence']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_words = [i for i in new_texts if i in positive]\n",
    "print(positive_words)\n",
    "len(positive_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b59646d",
   "metadata": {},
   "source": [
    "#### positive Score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d0fbab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n"
     ]
    }
   ],
   "source": [
    "pos_score = len(positive_words)\n",
    "print(pos_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a6cc12",
   "metadata": {},
   "source": [
    "#### Negative Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df0766cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    }
   ],
   "source": [
    "neg_score=len(negative_words)\n",
    "print(neg_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84da7ab7",
   "metadata": {},
   "source": [
    "#### Polarity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21e88c2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Polarity_Score = (pos_score - neg_score)/((pos_score + neg_score) + 0.000001)\n",
    "round(Polarity_Score,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08bcb4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words after cleaning : 279\n"
     ]
    }
   ],
   "source": [
    "word_count = len(new_texts)\n",
    "print(\"number of words after cleaning :\",word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16495ba1",
   "metadata": {},
   "source": [
    "#### Subjectivity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "841713a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Subjectivity_Score = (pos_score + neg_score)/ ((word_count) + 0.000001)\n",
    "round(Subjectivity_Score,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7c370e",
   "metadata": {},
   "source": [
    "# Analysis of Readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "10209a7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "505"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "word_tokens = nltk.word_tokenize(texts)\n",
    "No_of_words = len(word_tokens)\n",
    "No_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bfac2ab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokens = nltk.sent_tokenize(texts)\n",
    "No_of_sents = len(sent_tokens)\n",
    "No_of_sents "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1576acfe",
   "metadata": {},
   "source": [
    "### Average sentence Length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "11d6a7ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.58"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Avg_Sents_Length = No_of_words / No_of_sents\n",
    "round(Avg_Sents_Length,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc20ee3d",
   "metadata": {},
   "source": [
    "### Percentage of Complex words\n",
    "Complex words: words with more than 2 syllable are called complex words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eba6a8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of complex words: 92\n",
      "Total number of words: 2981\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import cmudict\n",
    "cmud = cmudict.dict()\n",
    "\n",
    "# Defining a function to count syllables in a word\n",
    "def count_syllables(word):\n",
    "    phonemes = cmud[word.lower()][0] \n",
    "    return len([s for s in phonemes if s[-1].isdigit()])\n",
    "\n",
    "# Identifing complex words\n",
    "w_tokens = [i for i in word_tokens if i in cmud]\n",
    "complex_words = [word for word in w_tokens if count_syllables(word) > 2]\n",
    "\n",
    "# Calculatingnumber of complex words\n",
    "num_complex_words = len(complex_words)\n",
    "\n",
    "print(\"Number of complex words:\", num_complex_words)\n",
    "print(\"Total number of words:\", len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e982c0a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.4"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Perc_of_Complex_words = len(texts) / num_complex_words\n",
    "round(Perc_of_Complex_words,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e948d1",
   "metadata": {},
   "source": [
    "### Fog Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "95fb10bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sentence length: 19.0\n"
     ]
    }
   ],
   "source": [
    "# Tokenize each sentence into words\n",
    "words = [word_tokens for sentence in sent_tokens]\n",
    "\n",
    "# Calculate the average sentence length\n",
    "avg_sent_len = sum(No_of_sents for sentence in words) / No_of_sents\n",
    "\n",
    "print(\"Average sentence length:\", avg_sent_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "38f5a93b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.56"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Fog_index = 0.4 * (avg_sent_len + Perc_of_Complex_words)\n",
    "round(Fog_index,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4aacea",
   "metadata": {},
   "source": [
    "# Average Number of Words Per Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "45da1407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.58"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_no_of_words_per_sent = No_of_words / No_of_sents\n",
    "round(avg_no_of_words_per_sent,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d9b493",
   "metadata": {},
   "source": [
    "# Complex Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "75304424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complex word count: 92\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import cmudict\n",
    "cmud = cmudict.dict()\n",
    "\n",
    "# Defining a function to count syllables in a word\n",
    "def count_syllables(word):\n",
    "    phonemes = cmud[word.lower()][0] \n",
    "    return len([s for s in phonemes if s[-1].isdigit()])\n",
    "\n",
    "# Identifing complex words\n",
    "w_tokens = [i for i in word_tokens if i in cmud]\n",
    "complex_words = [word for word in w_tokens if count_syllables(word) > 2]\n",
    "\n",
    "# Calculatingnumber of complex words\n",
    "num_complex_words = len(complex_words)\n",
    "\n",
    "print(\"complex word count:\", num_complex_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ea5188",
   "metadata": {},
   "source": [
    "# Word Count\n",
    "number of words after cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b4fb0c72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "279"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8076ff",
   "metadata": {},
   "source": [
    "# Syllable Count Per Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bc568045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a word to get syllable: precision\n",
      "Number of syllable in a word : 3\n"
     ]
    }
   ],
   "source": [
    "word = input(\"Enter a word to get syllable: \")\n",
    "def count_syllables(word):\n",
    "    phonemes = cmud[word.lower()][0] \n",
    "    return len([s for s in phonemes if s[-1].isdigit()])\n",
    "\n",
    "print(\"Number of syllable in a word :\",count_syllables(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3a18f6",
   "metadata": {},
   "source": [
    "# Personal Pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0b8e0d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Personal pronoun frequency: {'they': 3, 'them': 1, 'we': 1}\n",
      "total number of pronouns in a article : 5\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Define a regex pattern to match personal pronouns\n",
    "pattern = r'\\b(I|you|he|she|it|we|they|me|him|her|us|them)\\b'\n",
    "\n",
    "# Count the frequency of personal pronouns\n",
    "pronoun_freq = {}\n",
    "for pronoun in re.findall(pattern, texts, re.IGNORECASE):\n",
    "    pronoun = pronoun.lower()\n",
    "    if pronoun in pronoun_freq:\n",
    "        pronoun_freq[pronoun] += 1\n",
    "    else:\n",
    "        pronoun_freq[pronoun] = 1\n",
    "\n",
    "print(\"Personal pronoun frequency:\", pronoun_freq)\n",
    "def returnSum(dict):\n",
    " \n",
    "    sum = 0\n",
    "    for i in pronoun_freq.values():\n",
    "        sum = sum + i\n",
    " \n",
    "    return sum\n",
    "print(\"total number of pronouns in a article :\",returnSum(pronoun_freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fc87f8",
   "metadata": {},
   "source": [
    "# Average Word Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5bde37d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average word length: 4.89\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total number of characters in all words\n",
    "total_chars = sum(len(word) for word in word_tokens)\n",
    "\n",
    "avg_word_length = total_chars / No_of_words\n",
    "\n",
    "print(\"Average word length:\", round(avg_word_length,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dc25ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819012c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
