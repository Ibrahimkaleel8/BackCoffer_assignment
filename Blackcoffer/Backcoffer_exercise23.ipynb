{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9312f26",
   "metadata": {},
   "source": [
    "# Data scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef4785ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac2f2459",
   "metadata": {},
   "outputs": [],
   "source": [
    "req = requests.get(\"https://insights.blackcoffer.com/how-data-analytics-and-ai-are-used-to-halt-the-covid-19-pandemic/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45f9fa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(req.content, \"html.parser\")\n",
    "res = soup.title\n",
    "paras = soup.find_all('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbb256b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Even though COVID-19 has not yet halted and we are facing the nth wave of the coronavirus outbreak across several countries, most notably the US, India, and Brazil. It is a fact that Data Analytics and AI are the big guns of our artillery in this fight against the COVID-19 pandemic. It has helped us in several stages of this outbreak, like the detection of its first outbreak, vaccine development and manufacturing contact tracing, and future hotspot detection. Some of these interesting applications are discussed in this article. A lesser-known fact is that the COVID-19 outbreak was first detected in Toronto, Canada, nearly 7,230 miles away from the first outbreak, nine days before the WHO issued its warning. It was with the help of Big Data Analytics and AI, more specifically Deep Learnings (DL, a subset of Machine Learning) application in Natural Language Processing (NLP) to analyze text inputs that traced the surge of pneumonia cases in the Wuhan province of China. The specialty of DL algorithms is that they mimic the brain cells called neurons and can identify patterns in Big Data. This DL-backed software is used as inputs, reports from public health organizations, global airline ticketing data, etc. These were used to flag unusual surges and potential spreads of infectious diseases. The next application of Big Data Analytics and AI was in the Research and Development of drugs to halt COVID-19. AI was used to analyze the protein structure of the virus, findings that were significant in the progress of vaccine development. In preliminary studies, it was found that it does not mutate as fast as other viruses such as HIV, which means that a prophylactic vaccine is a better way to proceed rather than a therapy. But there is also some evidence supporting the fact that when we find any kind of cure for it, there is a chance of the virus mutating, which is what happened and major mutations have been found in the UK, Brazil, and South Africa. AI also assisted scientists in rapidly shortlisting a set of already available vaccines that could be effective against the coronavirus. Another interesting application of AI can be found in the selection of the right candidates, i.e. most likely to test positive for testing coronavirus in case of insufficient testing resources. This method was first exercised on Greek borders and was called project EVA. Whenever a traveler wanted to come into Greece, he had to fill out a form known as Passenger Locator Form (PLF) at least 24 hours prior to arrival, containing information on their origin country, demographics, point, and date of entry, and the intended destination. EVA then allocated testing resources according to the size of the set of passengers to be tested. After the test results, if found positive, they are put in quarantine. The results were sent back to the program for real-time learning. The question remains how EVA made allocations, It was found that, statistically, only the origin country and the city were significant factors for screening. Ultimately, from a variety of countries and city pairs, EVA had to predict how many testing resources were to be allocated at each entry point and to particular passengers from a location is technically called the Multi-Armed Bandit (MAB) problem, and the chosen method to solve this problem was an AI algorithm called optimistic Gittins index. This algorithm identified on average 1.85x as many asymptomatic, infected travelers as random surveillance testing, and up to 2-4x as many during peak travel. After the test results, if found positive, they are put in quarantine. Following the collection of significant data through the aforementioned process, after a certain period, policies were made categorizing them separately and imposing restrictions on travelers from the specific location. This EVA as presented above was in operation from August 6th to November 1st processing around 38,500 PLFs each day and testing on an average 18.5% of households entering the country every day. Above mentioned applications just show the tip of the iceberg and there is more to get into some of the other developments to watch for include the use of Image Recognition to identify covid based on x-ray images, the use of Deep Learning to predict the 3-D protein structure associated with COVID-19 and so on.\n"
     ]
    }
   ],
   "source": [
    "texts = \" \".join([paragraph.text.strip() for paragraph in paras])\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e79f5f60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4338"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8f1fcc",
   "metadata": {},
   "source": [
    "Removing Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5285c8a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4338"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "texts = re.sub(r'[\\'\\“\\”\\()\\%\\,\\-\\'\\’\\?\\ ]', ' ', texts)\n",
    "texts[0:200]\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fafe7be",
   "metadata": {},
   "source": [
    "# Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "089b8663",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "280876b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Even',\n",
       " 'though',\n",
       " 'COVID',\n",
       " '19',\n",
       " 'has',\n",
       " 'not',\n",
       " 'yet',\n",
       " 'halted',\n",
       " 'and',\n",
       " 'we',\n",
       " 'are',\n",
       " 'facing',\n",
       " 'the',\n",
       " 'nth',\n",
       " 'wave',\n",
       " 'of',\n",
       " 'the',\n",
       " 'coronavirus',\n",
       " 'outbreak',\n",
       " 'across',\n",
       " 'several',\n",
       " 'countries',\n",
       " 'most',\n",
       " 'notably',\n",
       " 'the',\n",
       " 'US',\n",
       " 'India',\n",
       " 'and',\n",
       " 'Brazil',\n",
       " '.',\n",
       " 'It',\n",
       " 'is',\n",
       " 'a',\n",
       " 'fact',\n",
       " 'that',\n",
       " 'Data',\n",
       " 'Analytics',\n",
       " 'and',\n",
       " 'AI',\n",
       " 'are',\n",
       " 'the',\n",
       " 'big',\n",
       " 'guns',\n",
       " 'of',\n",
       " 'our',\n",
       " 'artillery',\n",
       " 'in',\n",
       " 'this',\n",
       " 'fight',\n",
       " 'against',\n",
       " 'the',\n",
       " 'COVID',\n",
       " '19',\n",
       " 'pandemic',\n",
       " '.',\n",
       " 'It',\n",
       " 'has',\n",
       " 'helped',\n",
       " 'us',\n",
       " 'in',\n",
       " 'several',\n",
       " 'stages',\n",
       " 'of',\n",
       " 'this',\n",
       " 'outbreak',\n",
       " 'like',\n",
       " 'the',\n",
       " 'detection',\n",
       " 'of',\n",
       " 'its',\n",
       " 'first',\n",
       " 'outbreak',\n",
       " 'vaccine',\n",
       " 'development',\n",
       " 'and',\n",
       " 'manufacturing',\n",
       " 'contact',\n",
       " 'tracing',\n",
       " 'and',\n",
       " 'future',\n",
       " 'hotspot',\n",
       " 'detection',\n",
       " '.',\n",
       " 'Some',\n",
       " 'of',\n",
       " 'these',\n",
       " 'interesting',\n",
       " 'applications',\n",
       " 'are',\n",
       " 'discussed',\n",
       " 'in',\n",
       " 'this',\n",
       " 'article',\n",
       " '.',\n",
       " 'A',\n",
       " 'lesser',\n",
       " 'known',\n",
       " 'fact',\n",
       " 'is',\n",
       " 'that',\n",
       " 'the',\n",
       " 'COVID',\n",
       " '19',\n",
       " 'outbreak',\n",
       " 'was',\n",
       " 'first',\n",
       " 'detected',\n",
       " 'in',\n",
       " 'Toronto',\n",
       " 'Canada',\n",
       " 'nearly',\n",
       " '7',\n",
       " '230',\n",
       " 'miles',\n",
       " 'away',\n",
       " 'from',\n",
       " 'the',\n",
       " 'first',\n",
       " 'outbreak',\n",
       " 'nine',\n",
       " 'days',\n",
       " 'before',\n",
       " 'the',\n",
       " 'WHO',\n",
       " 'issued',\n",
       " 'its',\n",
       " 'warning',\n",
       " '.',\n",
       " 'It',\n",
       " 'was',\n",
       " 'with',\n",
       " 'the',\n",
       " 'help',\n",
       " 'of',\n",
       " 'Big',\n",
       " 'Data',\n",
       " 'Analytics',\n",
       " 'and',\n",
       " 'AI',\n",
       " 'more',\n",
       " 'specifically',\n",
       " 'Deep',\n",
       " 'Learnings',\n",
       " 'DL',\n",
       " 'a',\n",
       " 'subset',\n",
       " 'of',\n",
       " 'Machine',\n",
       " 'Learning',\n",
       " 'application',\n",
       " 'in',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " 'NLP',\n",
       " 'to',\n",
       " 'analyze',\n",
       " 'text',\n",
       " 'inputs',\n",
       " 'that',\n",
       " 'traced',\n",
       " 'the',\n",
       " 'surge',\n",
       " 'of',\n",
       " 'pneumonia',\n",
       " 'cases',\n",
       " 'in',\n",
       " 'the',\n",
       " 'Wuhan',\n",
       " 'province',\n",
       " 'of',\n",
       " 'China',\n",
       " '.',\n",
       " 'The',\n",
       " 'specialty',\n",
       " 'of',\n",
       " 'DL',\n",
       " 'algorithms',\n",
       " 'is',\n",
       " 'that',\n",
       " 'they',\n",
       " 'mimic',\n",
       " 'the',\n",
       " 'brain',\n",
       " 'cells',\n",
       " 'called',\n",
       " 'neurons',\n",
       " 'and',\n",
       " 'can',\n",
       " 'identify',\n",
       " 'patterns',\n",
       " 'in',\n",
       " 'Big',\n",
       " 'Data',\n",
       " '.',\n",
       " 'This',\n",
       " 'DL',\n",
       " 'backed',\n",
       " 'software',\n",
       " 'is',\n",
       " 'used',\n",
       " 'as',\n",
       " 'inputs',\n",
       " 'reports',\n",
       " 'from',\n",
       " 'public',\n",
       " 'health',\n",
       " 'organizations',\n",
       " 'global',\n",
       " 'airline',\n",
       " 'ticketing',\n",
       " 'data',\n",
       " 'etc',\n",
       " '.',\n",
       " 'These',\n",
       " 'were',\n",
       " 'used',\n",
       " 'to',\n",
       " 'flag',\n",
       " 'unusual',\n",
       " 'surges',\n",
       " 'and',\n",
       " 'potential',\n",
       " 'spreads',\n",
       " 'of',\n",
       " 'infectious',\n",
       " 'diseases',\n",
       " '.',\n",
       " 'The',\n",
       " 'next',\n",
       " 'application',\n",
       " 'of',\n",
       " 'Big',\n",
       " 'Data',\n",
       " 'Analytics',\n",
       " 'and',\n",
       " 'AI',\n",
       " 'was',\n",
       " 'in',\n",
       " 'the',\n",
       " 'Research',\n",
       " 'and',\n",
       " 'Development',\n",
       " 'of',\n",
       " 'drugs',\n",
       " 'to',\n",
       " 'halt',\n",
       " 'COVID',\n",
       " '19',\n",
       " '.',\n",
       " 'AI',\n",
       " 'was',\n",
       " 'used',\n",
       " 'to',\n",
       " 'analyze',\n",
       " 'the',\n",
       " 'protein',\n",
       " 'structure',\n",
       " 'of',\n",
       " 'the',\n",
       " 'virus',\n",
       " 'findings',\n",
       " 'that',\n",
       " 'were',\n",
       " 'significant',\n",
       " 'in',\n",
       " 'the',\n",
       " 'progress',\n",
       " 'of',\n",
       " 'vaccine',\n",
       " 'development',\n",
       " '.',\n",
       " 'In',\n",
       " 'preliminary',\n",
       " 'studies',\n",
       " 'it',\n",
       " 'was',\n",
       " 'found',\n",
       " 'that',\n",
       " 'it',\n",
       " 'does',\n",
       " 'not',\n",
       " 'mutate',\n",
       " 'as',\n",
       " 'fast',\n",
       " 'as',\n",
       " 'other',\n",
       " 'viruses',\n",
       " 'such',\n",
       " 'as',\n",
       " 'HIV',\n",
       " 'which',\n",
       " 'means',\n",
       " 'that',\n",
       " 'a',\n",
       " 'prophylactic',\n",
       " 'vaccine',\n",
       " 'is',\n",
       " 'a',\n",
       " 'better',\n",
       " 'way',\n",
       " 'to',\n",
       " 'proceed',\n",
       " 'rather',\n",
       " 'than',\n",
       " 'a',\n",
       " 'therapy',\n",
       " '.',\n",
       " 'But',\n",
       " 'there',\n",
       " 'is',\n",
       " 'also',\n",
       " 'some',\n",
       " 'evidence',\n",
       " 'supporting',\n",
       " 'the',\n",
       " 'fact',\n",
       " 'that',\n",
       " 'when',\n",
       " 'we',\n",
       " 'find',\n",
       " 'any',\n",
       " 'kind',\n",
       " 'of',\n",
       " 'cure',\n",
       " 'for',\n",
       " 'it',\n",
       " 'there',\n",
       " 'is',\n",
       " 'a',\n",
       " 'chance',\n",
       " 'of',\n",
       " 'the',\n",
       " 'virus',\n",
       " 'mutating',\n",
       " 'which',\n",
       " 'is',\n",
       " 'what',\n",
       " 'happened',\n",
       " 'and',\n",
       " 'major',\n",
       " 'mutations',\n",
       " 'have',\n",
       " 'been',\n",
       " 'found',\n",
       " 'in',\n",
       " 'the',\n",
       " 'UK',\n",
       " 'Brazil',\n",
       " 'and',\n",
       " 'South',\n",
       " 'Africa',\n",
       " '.',\n",
       " 'AI',\n",
       " 'also',\n",
       " 'assisted',\n",
       " 'scientists',\n",
       " 'in',\n",
       " 'rapidly',\n",
       " 'shortlisting',\n",
       " 'a',\n",
       " 'set',\n",
       " 'of',\n",
       " 'already',\n",
       " 'available',\n",
       " 'vaccines',\n",
       " 'that',\n",
       " 'could',\n",
       " 'be',\n",
       " 'effective',\n",
       " 'against',\n",
       " 'the',\n",
       " 'coronavirus',\n",
       " '.',\n",
       " 'Another',\n",
       " 'interesting',\n",
       " 'application',\n",
       " 'of',\n",
       " 'AI',\n",
       " 'can',\n",
       " 'be',\n",
       " 'found',\n",
       " 'in',\n",
       " 'the',\n",
       " 'selection',\n",
       " 'of',\n",
       " 'the',\n",
       " 'right',\n",
       " 'candidates',\n",
       " 'i.e',\n",
       " '.',\n",
       " 'most',\n",
       " 'likely',\n",
       " 'to',\n",
       " 'test',\n",
       " 'positive',\n",
       " 'for',\n",
       " 'testing',\n",
       " 'coronavirus',\n",
       " 'in',\n",
       " 'case',\n",
       " 'of',\n",
       " 'insufficient',\n",
       " 'testing',\n",
       " 'resources',\n",
       " '.',\n",
       " 'This',\n",
       " 'method',\n",
       " 'was',\n",
       " 'first',\n",
       " 'exercised',\n",
       " 'on',\n",
       " 'Greek',\n",
       " 'borders',\n",
       " 'and',\n",
       " 'was',\n",
       " 'called',\n",
       " 'project',\n",
       " 'EVA',\n",
       " '.',\n",
       " 'Whenever',\n",
       " 'a',\n",
       " 'traveler',\n",
       " 'wanted',\n",
       " 'to',\n",
       " 'come',\n",
       " 'into',\n",
       " 'Greece',\n",
       " 'he',\n",
       " 'had',\n",
       " 'to',\n",
       " 'fill',\n",
       " 'out',\n",
       " 'a',\n",
       " 'form',\n",
       " 'known',\n",
       " 'as',\n",
       " 'Passenger',\n",
       " 'Locator',\n",
       " 'Form',\n",
       " 'PLF',\n",
       " 'at',\n",
       " 'least',\n",
       " '24',\n",
       " 'hours',\n",
       " 'prior',\n",
       " 'to',\n",
       " 'arrival',\n",
       " 'containing',\n",
       " 'information',\n",
       " 'on',\n",
       " 'their',\n",
       " 'origin',\n",
       " 'country',\n",
       " 'demographics',\n",
       " 'point',\n",
       " 'and',\n",
       " 'date',\n",
       " 'of',\n",
       " 'entry',\n",
       " 'and',\n",
       " 'the',\n",
       " 'intended',\n",
       " 'destination',\n",
       " '.',\n",
       " 'EVA',\n",
       " 'then',\n",
       " 'allocated',\n",
       " 'testing',\n",
       " 'resources',\n",
       " 'according',\n",
       " 'to',\n",
       " 'the',\n",
       " 'size',\n",
       " 'of',\n",
       " 'the',\n",
       " 'set',\n",
       " 'of',\n",
       " 'passengers',\n",
       " 'to',\n",
       " 'be',\n",
       " 'tested',\n",
       " '.',\n",
       " 'After',\n",
       " 'the',\n",
       " 'test',\n",
       " 'results',\n",
       " 'if',\n",
       " 'found',\n",
       " 'positive',\n",
       " 'they',\n",
       " 'are',\n",
       " 'put',\n",
       " 'in',\n",
       " 'quarantine',\n",
       " '.',\n",
       " 'The',\n",
       " 'results',\n",
       " 'were',\n",
       " 'sent',\n",
       " 'back',\n",
       " 'to',\n",
       " 'the',\n",
       " 'program',\n",
       " 'for',\n",
       " 'real',\n",
       " 'time',\n",
       " 'learning',\n",
       " '.',\n",
       " 'The',\n",
       " 'question',\n",
       " 'remains',\n",
       " 'how',\n",
       " 'EVA',\n",
       " 'made',\n",
       " 'allocations',\n",
       " 'It',\n",
       " 'was',\n",
       " 'found',\n",
       " 'that',\n",
       " 'statistically',\n",
       " 'only',\n",
       " 'the',\n",
       " 'origin',\n",
       " 'country',\n",
       " 'and',\n",
       " 'the',\n",
       " 'city',\n",
       " 'were',\n",
       " 'significant',\n",
       " 'factors',\n",
       " 'for',\n",
       " 'screening',\n",
       " '.',\n",
       " 'Ultimately',\n",
       " 'from',\n",
       " 'a',\n",
       " 'variety',\n",
       " 'of',\n",
       " 'countries',\n",
       " 'and',\n",
       " 'city',\n",
       " 'pairs',\n",
       " 'EVA',\n",
       " 'had',\n",
       " 'to',\n",
       " 'predict',\n",
       " 'how',\n",
       " 'many',\n",
       " 'testing',\n",
       " 'resources',\n",
       " 'were',\n",
       " 'to',\n",
       " 'be',\n",
       " 'allocated',\n",
       " 'at',\n",
       " 'each',\n",
       " 'entry',\n",
       " 'point',\n",
       " 'and',\n",
       " 'to',\n",
       " 'particular',\n",
       " 'passengers',\n",
       " 'from',\n",
       " 'a',\n",
       " 'location',\n",
       " 'is',\n",
       " 'technically',\n",
       " 'called',\n",
       " 'the',\n",
       " 'Multi',\n",
       " 'Armed',\n",
       " 'Bandit',\n",
       " 'MAB',\n",
       " 'problem',\n",
       " 'and',\n",
       " 'the',\n",
       " 'chosen',\n",
       " 'method',\n",
       " 'to',\n",
       " 'solve',\n",
       " 'this',\n",
       " 'problem',\n",
       " 'was',\n",
       " 'an',\n",
       " 'AI',\n",
       " 'algorithm',\n",
       " 'called',\n",
       " 'optimistic',\n",
       " 'Gittins',\n",
       " 'index',\n",
       " '.',\n",
       " 'This',\n",
       " 'algorithm',\n",
       " 'identified',\n",
       " 'on',\n",
       " 'average',\n",
       " '1.85x',\n",
       " 'as',\n",
       " 'many',\n",
       " 'asymptomatic',\n",
       " 'infected',\n",
       " 'travelers',\n",
       " 'as',\n",
       " 'random',\n",
       " 'surveillance',\n",
       " 'testing',\n",
       " 'and',\n",
       " 'up',\n",
       " 'to',\n",
       " '2',\n",
       " '4x',\n",
       " 'as',\n",
       " 'many',\n",
       " 'during',\n",
       " 'peak',\n",
       " 'travel',\n",
       " '.',\n",
       " 'After',\n",
       " 'the',\n",
       " 'test',\n",
       " 'results',\n",
       " 'if',\n",
       " 'found',\n",
       " 'positive',\n",
       " 'they',\n",
       " 'are',\n",
       " 'put',\n",
       " 'in',\n",
       " 'quarantine',\n",
       " '.',\n",
       " 'Following',\n",
       " 'the',\n",
       " 'collection',\n",
       " 'of',\n",
       " 'significant',\n",
       " 'data',\n",
       " 'through',\n",
       " 'the',\n",
       " 'aforementioned',\n",
       " 'process',\n",
       " 'after',\n",
       " 'a',\n",
       " 'certain',\n",
       " 'period',\n",
       " 'policies',\n",
       " 'were',\n",
       " 'made',\n",
       " 'categorizing',\n",
       " 'them',\n",
       " 'separately',\n",
       " 'and',\n",
       " 'imposing',\n",
       " 'restrictions',\n",
       " 'on',\n",
       " 'travelers',\n",
       " 'from',\n",
       " 'the',\n",
       " 'specific',\n",
       " 'location',\n",
       " '.',\n",
       " 'This',\n",
       " 'EVA',\n",
       " 'as',\n",
       " 'presented',\n",
       " 'above',\n",
       " 'was',\n",
       " 'in',\n",
       " 'operation',\n",
       " 'from',\n",
       " 'August',\n",
       " '6th',\n",
       " 'to',\n",
       " 'November',\n",
       " '1st',\n",
       " 'processing',\n",
       " 'around',\n",
       " '38',\n",
       " '500',\n",
       " 'PLFs',\n",
       " 'each',\n",
       " 'day',\n",
       " 'and',\n",
       " 'testing',\n",
       " 'on',\n",
       " 'an',\n",
       " 'average',\n",
       " '18.5',\n",
       " 'of',\n",
       " 'households',\n",
       " 'entering',\n",
       " 'the',\n",
       " 'country',\n",
       " 'every',\n",
       " 'day',\n",
       " '.',\n",
       " 'Above',\n",
       " 'mentioned',\n",
       " 'applications',\n",
       " 'just',\n",
       " 'show',\n",
       " 'the',\n",
       " 'tip',\n",
       " 'of',\n",
       " 'the',\n",
       " 'iceberg',\n",
       " 'and',\n",
       " 'there',\n",
       " 'is',\n",
       " 'more',\n",
       " 'to',\n",
       " 'get',\n",
       " 'into',\n",
       " 'some',\n",
       " 'of',\n",
       " 'the',\n",
       " 'other',\n",
       " 'developments',\n",
       " 'to',\n",
       " 'watch',\n",
       " 'for',\n",
       " 'include',\n",
       " 'the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'Image',\n",
       " 'Recognition',\n",
       " 'to',\n",
       " 'identify',\n",
       " 'covid',\n",
       " 'based',\n",
       " 'on',\n",
       " 'x',\n",
       " 'ray',\n",
       " 'images',\n",
       " 'the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'Deep',\n",
       " 'Learning',\n",
       " 'to',\n",
       " 'predict',\n",
       " 'the',\n",
       " '3',\n",
       " 'D',\n",
       " 'protein',\n",
       " 'structure',\n",
       " 'associated',\n",
       " 'with',\n",
       " 'COVID',\n",
       " '19',\n",
       " 'and',\n",
       " 'so',\n",
       " 'on',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = word_tokenize(texts)\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10016a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"D:\\lab\\Dataset\\Stopwords_Blackcoffer.txt\", 'r') as file:\n",
    "    data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c567cbef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ERNST\\nYOUNG\\nDELOITTE\\nTOUCHE\\nKPMG\\nPRICEWATERHOUSECOOPERS\\nPRICEWATERHOUSE\\nCOOPERS\\nAFGHANI\\nARIARY\\nBAHT\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d65bfa92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ERNST,YOUNG,DELOITTE,TOUCHE,KPMG,PRICEWATERHOUSECOOPERS,PRICEWATERHOUSE,COOPERS,AFGHANI,ARIARY,BAHT,'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.replace('\\n', ',')\n",
    "data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ef23874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Even COVID 19 halted facing nth wave coronavirus outbreak countries notably India Brazil . It fact Data Analytics big guns artillery fight COVID 19 pandemic . It helped stages outbreak detection outbr'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_texts = \" \".join([i for i in token if i not in data])\n",
    "new_texts[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea392da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length before cleaning:  4338\n",
      "length after cleaning:  2834\n"
     ]
    }
   ],
   "source": [
    "print(\"length before cleaning: \", len(texts))\n",
    "print(\"length after cleaning: \", len(new_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f456147",
   "metadata": {},
   "source": [
    "# Negative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc3618a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"D:\\\\lab\\\\Dataset\\\\Negative_words.txt\", 'r') as file:\n",
    "    negative = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2123c4cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2-faced\\n2-faces\\nabnormal\\nabolish\\nabominable\\nabominably\\nabominate\\nabomination\\nabort\\naborted\\naborts\\nabrade\\nabrasive\\nabrupt\\nabruptly\\nabscond\\nabsence\\nabsent-minded\\nabsentee\\nabsurd\\nabsurdity\\nabsurdly\\nabsurdness\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative[:206]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28f44c97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2-faced,2-faces,abnormal,abolish,abominable,abominably,abominate,abomination,abort,aborted,aborts,abrade,abrasive,abrupt,abruptly,abscond,absence,absent-minded,absentee,absurd,absurdity,absurdly,absurdness,'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative = negative.replace('\\n', ',')\n",
    "negative[:206]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31a272c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nth', 'outbreak', 'fact', 'big', 'outbreak', 'outbreak', 'lesser', 'fact', 'outbreak', 'outbreak', 'warning', 'surge', 'brain', 'health', 'flag', 'unusual', 'virus', 'significant', 'found', 'fast', 'fact', 'kind', 'cure', 'virus', 'found', 'set', 'effective', 'found', 'test', 'testing', 'insufficient', 'testing', 'wanted', 'fill', 'point', 'date', 'testing', 'size', 'set', 'tested', 'test', 'found', 'put', 'back', 'question', 'found', 'city', 'significant', 'city', 'predict', 'testing', 'point', 'problem', 'solve', 'problem', 'infected', 'random', 'testing', '2', 'peak', 'test', 'found', 'put', 'significant', 'imposing', 'day', 'testing', 'day', 'show', 'tip', 'watch', 'ray', 'predict']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_texts = word_tokenize(new_texts)\n",
    "negative_words = [i for i in new_texts if i in negative]\n",
    "print(negative_words)\n",
    "len(negative_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b24f011",
   "metadata": {},
   "source": [
    "# Positive words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b003620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a+\\nabound\\nabounds\\nabundance\\nabundant\\naccessable\\naccessible\\nacclaim\\nacclaimed\\nacclamation\\naccolade\\naccolades\\naccommodative\\naccomodative\\naccomplish\\naccomplished\\naccomplishment\\naccomplishments\\naccurate\\naccurately\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"D:\\\\lab\\\\Dataset\\\\Positive_words.txt\", 'r') as file:\n",
    "    positive = file.read()\n",
    "positive[:210]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66ed38c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a+,abound,abounds,abundance,abundant,accessable,accessible,acclaim,acclaimed,acclamation,accolade,accolades,accommodative,accomodative,accomplish,accomplished,accomplishment,accomplishments,accurate,accurately,'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive = positive.replace('\\n',',')\n",
    "positive[:210]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc1387e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nth', 'wave', 'notably', 'fact', 'helped', 'future', 'interesting', 'fact', 'miles', 'brain', 'health', 'structure', 'significant', 'progress', 'found', 'fast', 'supporting', 'fact', 'kind', 'cure', 'found', 'set', 'effective', 'interesting', 'found', 'test', 'positive', 'fill', 'origin', 'set', 'test', 'found', 'positive', 'put', 'back', 'question', 'made', 'found', 'origin', 'city', 'significant', 'variety', 'city', 'problem', 'solve', 'problem', 'optimistic', 'test', 'found', 'positive', 'put', 'significant', 'made', 'ray', 'structure']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_words = [i for i in new_texts if i in positive]\n",
    "print(positive_words)\n",
    "len(positive_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b59646d",
   "metadata": {},
   "source": [
    "#### positive Score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d0fbab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    }
   ],
   "source": [
    "pos_score = len(positive_words)\n",
    "print(pos_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a6cc12",
   "metadata": {},
   "source": [
    "#### Negative Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df0766cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73\n"
     ]
    }
   ],
   "source": [
    "neg_score=len(negative_words)\n",
    "print(neg_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84da7ab7",
   "metadata": {},
   "source": [
    "#### Polarity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21e88c2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.14"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Polarity_Score = (pos_score - neg_score)/((pos_score + neg_score) + 0.000001)\n",
    "round(Polarity_Score,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08bcb4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words after cleaning : 397\n"
     ]
    }
   ],
   "source": [
    "word_count = len(new_texts)\n",
    "print(\"number of words after cleaning :\",word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16495ba1",
   "metadata": {},
   "source": [
    "#### Subjectivity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "841713a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Subjectivity_Score = (pos_score + neg_score)/ ((word_count) + 0.000001)\n",
    "round(Subjectivity_Score,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7c370e",
   "metadata": {},
   "source": [
    "# Analysis of Readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "10209a7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "755"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "word_tokens = nltk.word_tokenize(texts)\n",
    "No_of_words = len(word_tokens)\n",
    "No_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bfac2ab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokens = nltk.sent_tokenize(texts)\n",
    "No_of_sents = len(sent_tokens)\n",
    "No_of_sents "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1576acfe",
   "metadata": {},
   "source": [
    "### Average sentence Length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "11d6a7ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.96"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Avg_Sents_Length = No_of_words / No_of_sents\n",
    "round(Avg_Sents_Length,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc20ee3d",
   "metadata": {},
   "source": [
    "### Percentage of Complex words\n",
    "Complex words: words with more than 2 syllable are called complex words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eba6a8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of complex words: 110\n",
      "Total number of words: 4338\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import cmudict\n",
    "cmud = cmudict.dict()\n",
    "\n",
    "# Defining a function to count syllables in a word\n",
    "def count_syllables(word):\n",
    "    phonemes = cmud[word.lower()][0] \n",
    "    return len([s for s in phonemes if s[-1].isdigit()])\n",
    "\n",
    "# Identifing complex words\n",
    "w_tokens = [i for i in word_tokens if i in cmud]\n",
    "complex_words = [word for word in w_tokens if count_syllables(word) > 2]\n",
    "\n",
    "# Calculatingnumber of complex words\n",
    "num_complex_words = len(complex_words)\n",
    "\n",
    "print(\"Number of complex words:\", num_complex_words)\n",
    "print(\"Total number of words:\", len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e982c0a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39.44"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Perc_of_Complex_words = len(texts) / num_complex_words\n",
    "round(Perc_of_Complex_words,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e948d1",
   "metadata": {},
   "source": [
    "### Fog Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "95fb10bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sentence length: 28.0\n"
     ]
    }
   ],
   "source": [
    "# Tokenize each sentence into words\n",
    "words = [word_tokens for sentence in sent_tokens]\n",
    "\n",
    "# Calculate the average sentence length\n",
    "avg_sent_len = sum(No_of_sents for sentence in words) / No_of_sents\n",
    "\n",
    "print(\"Average sentence length:\", avg_sent_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "38f5a93b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.97"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Fog_index = 0.4 * (avg_sent_len + Perc_of_Complex_words)\n",
    "round(Fog_index,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4aacea",
   "metadata": {},
   "source": [
    "# Average Number of Words Per Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "45da1407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.96"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_no_of_words_per_sent = No_of_words / No_of_sents\n",
    "round(avg_no_of_words_per_sent,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d9b493",
   "metadata": {},
   "source": [
    "# Complex Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "75304424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complex word count: 110\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import cmudict\n",
    "cmud = cmudict.dict()\n",
    "\n",
    "# Defining a function to count syllables in a word\n",
    "def count_syllables(word):\n",
    "    phonemes = cmud[word.lower()][0] \n",
    "    return len([s for s in phonemes if s[-1].isdigit()])\n",
    "\n",
    "# Identifing complex words\n",
    "w_tokens = [i for i in word_tokens if i in cmud]\n",
    "complex_words = [word for word in w_tokens if count_syllables(word) > 2]\n",
    "\n",
    "# Calculatingnumber of complex words\n",
    "num_complex_words = len(complex_words)\n",
    "\n",
    "print(\"complex word count:\", num_complex_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ea5188",
   "metadata": {},
   "source": [
    "# Word Count\n",
    "number of words after cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b4fb0c72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "397"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8076ff",
   "metadata": {},
   "source": [
    "# Syllable Count Per Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bc568045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a word to get syllable: detected\n",
      "Number of syllable in a word : 3\n"
     ]
    }
   ],
   "source": [
    "word = input(\"Enter a word to get syllable: \")\n",
    "def count_syllables(word):\n",
    "    phonemes = cmud[word.lower()][0] \n",
    "    return len([s for s in phonemes if s[-1].isdigit()])\n",
    "\n",
    "print(\"Number of syllable in a word :\",count_syllables(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3a18f6",
   "metadata": {},
   "source": [
    "# Personal Pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0b8e0d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Personal pronoun frequency: {'we': 2, 'us': 2, 'it': 7, 'they': 3, 'i': 1, 'he': 1, 'them': 1}\n",
      "total number of pronouns in a article : 17\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Define a regex pattern to match personal pronouns\n",
    "pattern = r'\\b(I|you|he|she|it|we|they|me|him|her|us|them)\\b'\n",
    "\n",
    "# Count the frequency of personal pronouns\n",
    "pronoun_freq = {}\n",
    "for pronoun in re.findall(pattern, texts, re.IGNORECASE):\n",
    "    pronoun = pronoun.lower()\n",
    "    if pronoun in pronoun_freq:\n",
    "        pronoun_freq[pronoun] += 1\n",
    "    else:\n",
    "        pronoun_freq[pronoun] = 1\n",
    "\n",
    "print(\"Personal pronoun frequency:\", pronoun_freq)\n",
    "def returnSum(dict):\n",
    " \n",
    "    sum = 0\n",
    "    for i in pronoun_freq.values():\n",
    "        sum = sum + i\n",
    " \n",
    "    return sum\n",
    "print(\"total number of pronouns in a article :\",returnSum(pronoun_freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fc87f8",
   "metadata": {},
   "source": [
    "# Average Word Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5bde37d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average word length: 4.72\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total number of characters in all words\n",
    "total_chars = sum(len(word) for word in word_tokens)\n",
    "\n",
    "avg_word_length = total_chars / No_of_words\n",
    "\n",
    "print(\"Average word length:\", round(avg_word_length,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dc25ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819012c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
