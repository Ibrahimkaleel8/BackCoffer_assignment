{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9312f26",
   "metadata": {},
   "source": [
    "# Data scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef4785ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac2f2459",
   "metadata": {},
   "outputs": [],
   "source": [
    "req = requests.get(\"https://insights.blackcoffer.com/will-technology-eliminate-the-need-for-animal-testing-in-drug-development/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45f9fa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(req.content, \"html.parser\")\n",
    "res = soup.title\n",
    "paras = soup.find_all('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbb256b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Imagine yourself as a being who can speak but no one can comprehend you. You are kept in captivity, in a place where you can hardly breathe. You are finally selected and taken out of that place, it is a happy moment for you. You can finally have your freedom but sadly instead of being free, you are prodded by rods and fed toxic substances against your will. The only thing you can do is scream in vain. Your voice is heard by everyone but it gets lost in the noise of scientific advancement. You writhe in pain as the substances take effect on your body, the pain slowly creeps into your body and you slowly succumb to it, never to wake up again. Since the time of Aristotle, animals have been subjected to various inhumane experiments for the sake of knowledge. These experiments though unethical were very necessary for the advancement of science. Animal testing has become an important part of experimenting on living organisms to ensure that a product is safe for humans to consume. The lab animals are generally subjected to high levels of toxicity while being kept in isolation due to which they have to suffer a lot of stress and discomfort. Animal testing has a large number of drawbacks: All of these things make us question ourselves. Is it even worth it? Is it possible to stop this cruelty without hampering our growth? With the advancement in science, it can be made possible that we don’t have to use animals ever again. These bio-printed models can also be used by the researchers to conduct experiments on them without using animals for it. A French company is working on a bio-printed liver model to test liver toxicity without having to use animals at all. This is a breakthrough in the field of bio-technology as it can bring down animal experimentation by a large percentage. These alternatives can help in bringing down animal experimentation to a large extent. If science can progress without harming animals then we should go for it rather than putting them through excruciating pain. We must protect the animals as much as we can or else we would soon be living in a world without them.\n"
     ]
    }
   ],
   "source": [
    "texts = \" \".join([paragraph.text.strip() for paragraph in paras])\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e79f5f60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2113"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8f1fcc",
   "metadata": {},
   "source": [
    "Removing Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5285c8a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2113"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "texts = re.sub(r'[\\'\\“\\”\\()\\%\\,\\-\\'\\’\\?\\ ]', ' ', texts)\n",
    "texts[0:200]\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fafe7be",
   "metadata": {},
   "source": [
    "# Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "089b8663",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "280876b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Imagine',\n",
       " 'yourself',\n",
       " 'as',\n",
       " 'a',\n",
       " 'being',\n",
       " 'who',\n",
       " 'can',\n",
       " 'speak',\n",
       " 'but',\n",
       " 'no',\n",
       " 'one',\n",
       " 'can',\n",
       " 'comprehend',\n",
       " 'you',\n",
       " '.',\n",
       " 'You',\n",
       " 'are',\n",
       " 'kept',\n",
       " 'in',\n",
       " 'captivity',\n",
       " 'in',\n",
       " 'a',\n",
       " 'place',\n",
       " 'where',\n",
       " 'you',\n",
       " 'can',\n",
       " 'hardly',\n",
       " 'breathe',\n",
       " '.',\n",
       " 'You',\n",
       " 'are',\n",
       " 'finally',\n",
       " 'selected',\n",
       " 'and',\n",
       " 'taken',\n",
       " 'out',\n",
       " 'of',\n",
       " 'that',\n",
       " 'place',\n",
       " 'it',\n",
       " 'is',\n",
       " 'a',\n",
       " 'happy',\n",
       " 'moment',\n",
       " 'for',\n",
       " 'you',\n",
       " '.',\n",
       " 'You',\n",
       " 'can',\n",
       " 'finally',\n",
       " 'have',\n",
       " 'your',\n",
       " 'freedom',\n",
       " 'but',\n",
       " 'sadly',\n",
       " 'instead',\n",
       " 'of',\n",
       " 'being',\n",
       " 'free',\n",
       " 'you',\n",
       " 'are',\n",
       " 'prodded',\n",
       " 'by',\n",
       " 'rods',\n",
       " 'and',\n",
       " 'fed',\n",
       " 'toxic',\n",
       " 'substances',\n",
       " 'against',\n",
       " 'your',\n",
       " 'will',\n",
       " '.',\n",
       " 'The',\n",
       " 'only',\n",
       " 'thing',\n",
       " 'you',\n",
       " 'can',\n",
       " 'do',\n",
       " 'is',\n",
       " 'scream',\n",
       " 'in',\n",
       " 'vain',\n",
       " '.',\n",
       " 'Your',\n",
       " 'voice',\n",
       " 'is',\n",
       " 'heard',\n",
       " 'by',\n",
       " 'everyone',\n",
       " 'but',\n",
       " 'it',\n",
       " 'gets',\n",
       " 'lost',\n",
       " 'in',\n",
       " 'the',\n",
       " 'noise',\n",
       " 'of',\n",
       " 'scientific',\n",
       " 'advancement',\n",
       " '.',\n",
       " 'You',\n",
       " 'writhe',\n",
       " 'in',\n",
       " 'pain',\n",
       " 'as',\n",
       " 'the',\n",
       " 'substances',\n",
       " 'take',\n",
       " 'effect',\n",
       " 'on',\n",
       " 'your',\n",
       " 'body',\n",
       " 'the',\n",
       " 'pain',\n",
       " 'slowly',\n",
       " 'creeps',\n",
       " 'into',\n",
       " 'your',\n",
       " 'body',\n",
       " 'and',\n",
       " 'you',\n",
       " 'slowly',\n",
       " 'succumb',\n",
       " 'to',\n",
       " 'it',\n",
       " 'never',\n",
       " 'to',\n",
       " 'wake',\n",
       " 'up',\n",
       " 'again',\n",
       " '.',\n",
       " 'Since',\n",
       " 'the',\n",
       " 'time',\n",
       " 'of',\n",
       " 'Aristotle',\n",
       " 'animals',\n",
       " 'have',\n",
       " 'been',\n",
       " 'subjected',\n",
       " 'to',\n",
       " 'various',\n",
       " 'inhumane',\n",
       " 'experiments',\n",
       " 'for',\n",
       " 'the',\n",
       " 'sake',\n",
       " 'of',\n",
       " 'knowledge',\n",
       " '.',\n",
       " 'These',\n",
       " 'experiments',\n",
       " 'though',\n",
       " 'unethical',\n",
       " 'were',\n",
       " 'very',\n",
       " 'necessary',\n",
       " 'for',\n",
       " 'the',\n",
       " 'advancement',\n",
       " 'of',\n",
       " 'science',\n",
       " '.',\n",
       " 'Animal',\n",
       " 'testing',\n",
       " 'has',\n",
       " 'become',\n",
       " 'an',\n",
       " 'important',\n",
       " 'part',\n",
       " 'of',\n",
       " 'experimenting',\n",
       " 'on',\n",
       " 'living',\n",
       " 'organisms',\n",
       " 'to',\n",
       " 'ensure',\n",
       " 'that',\n",
       " 'a',\n",
       " 'product',\n",
       " 'is',\n",
       " 'safe',\n",
       " 'for',\n",
       " 'humans',\n",
       " 'to',\n",
       " 'consume',\n",
       " '.',\n",
       " 'The',\n",
       " 'lab',\n",
       " 'animals',\n",
       " 'are',\n",
       " 'generally',\n",
       " 'subjected',\n",
       " 'to',\n",
       " 'high',\n",
       " 'levels',\n",
       " 'of',\n",
       " 'toxicity',\n",
       " 'while',\n",
       " 'being',\n",
       " 'kept',\n",
       " 'in',\n",
       " 'isolation',\n",
       " 'due',\n",
       " 'to',\n",
       " 'which',\n",
       " 'they',\n",
       " 'have',\n",
       " 'to',\n",
       " 'suffer',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'of',\n",
       " 'stress',\n",
       " 'and',\n",
       " 'discomfort',\n",
       " '.',\n",
       " 'Animal',\n",
       " 'testing',\n",
       " 'has',\n",
       " 'a',\n",
       " 'large',\n",
       " 'number',\n",
       " 'of',\n",
       " 'drawbacks',\n",
       " ':',\n",
       " 'All',\n",
       " 'of',\n",
       " 'these',\n",
       " 'things',\n",
       " 'make',\n",
       " 'us',\n",
       " 'question',\n",
       " 'ourselves',\n",
       " '.',\n",
       " 'Is',\n",
       " 'it',\n",
       " 'even',\n",
       " 'worth',\n",
       " 'it',\n",
       " 'Is',\n",
       " 'it',\n",
       " 'possible',\n",
       " 'to',\n",
       " 'stop',\n",
       " 'this',\n",
       " 'cruelty',\n",
       " 'without',\n",
       " 'hampering',\n",
       " 'our',\n",
       " 'growth',\n",
       " 'With',\n",
       " 'the',\n",
       " 'advancement',\n",
       " 'in',\n",
       " 'science',\n",
       " 'it',\n",
       " 'can',\n",
       " 'be',\n",
       " 'made',\n",
       " 'possible',\n",
       " 'that',\n",
       " 'we',\n",
       " 'don',\n",
       " 't',\n",
       " 'have',\n",
       " 'to',\n",
       " 'use',\n",
       " 'animals',\n",
       " 'ever',\n",
       " 'again',\n",
       " '.',\n",
       " 'These',\n",
       " 'bio',\n",
       " 'printed',\n",
       " 'models',\n",
       " 'can',\n",
       " 'also',\n",
       " 'be',\n",
       " 'used',\n",
       " 'by',\n",
       " 'the',\n",
       " 'researchers',\n",
       " 'to',\n",
       " 'conduct',\n",
       " 'experiments',\n",
       " 'on',\n",
       " 'them',\n",
       " 'without',\n",
       " 'using',\n",
       " 'animals',\n",
       " 'for',\n",
       " 'it',\n",
       " '.',\n",
       " 'A',\n",
       " 'French',\n",
       " 'company',\n",
       " 'is',\n",
       " 'working',\n",
       " 'on',\n",
       " 'a',\n",
       " 'bio',\n",
       " 'printed',\n",
       " 'liver',\n",
       " 'model',\n",
       " 'to',\n",
       " 'test',\n",
       " 'liver',\n",
       " 'toxicity',\n",
       " 'without',\n",
       " 'having',\n",
       " 'to',\n",
       " 'use',\n",
       " 'animals',\n",
       " 'at',\n",
       " 'all',\n",
       " '.',\n",
       " 'This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'breakthrough',\n",
       " 'in',\n",
       " 'the',\n",
       " 'field',\n",
       " 'of',\n",
       " 'bio',\n",
       " 'technology',\n",
       " 'as',\n",
       " 'it',\n",
       " 'can',\n",
       " 'bring',\n",
       " 'down',\n",
       " 'animal',\n",
       " 'experimentation',\n",
       " 'by',\n",
       " 'a',\n",
       " 'large',\n",
       " 'percentage',\n",
       " '.',\n",
       " 'These',\n",
       " 'alternatives',\n",
       " 'can',\n",
       " 'help',\n",
       " 'in',\n",
       " 'bringing',\n",
       " 'down',\n",
       " 'animal',\n",
       " 'experimentation',\n",
       " 'to',\n",
       " 'a',\n",
       " 'large',\n",
       " 'extent',\n",
       " '.',\n",
       " 'If',\n",
       " 'science',\n",
       " 'can',\n",
       " 'progress',\n",
       " 'without',\n",
       " 'harming',\n",
       " 'animals',\n",
       " 'then',\n",
       " 'we',\n",
       " 'should',\n",
       " 'go',\n",
       " 'for',\n",
       " 'it',\n",
       " 'rather',\n",
       " 'than',\n",
       " 'putting',\n",
       " 'them',\n",
       " 'through',\n",
       " 'excruciating',\n",
       " 'pain',\n",
       " '.',\n",
       " 'We',\n",
       " 'must',\n",
       " 'protect',\n",
       " 'the',\n",
       " 'animals',\n",
       " 'as',\n",
       " 'much',\n",
       " 'as',\n",
       " 'we',\n",
       " 'can',\n",
       " 'or',\n",
       " 'else',\n",
       " 'we',\n",
       " 'would',\n",
       " 'soon',\n",
       " 'be',\n",
       " 'living',\n",
       " 'in',\n",
       " 'a',\n",
       " 'world',\n",
       " 'without',\n",
       " 'them',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = word_tokenize(texts)\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10016a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"D:\\lab\\Dataset\\Stopwords_Blackcoffer.txt\", 'r') as file:\n",
    "    data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c567cbef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ERNST\\nYOUNG\\nDELOITTE\\nTOUCHE\\nKPMG\\nPRICEWATERHOUSECOOPERS\\nPRICEWATERHOUSE\\nCOOPERS\\nAFGHANI\\nARIARY\\nBAHT\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d65bfa92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ERNST,YOUNG,DELOITTE,TOUCHE,KPMG,PRICEWATERHOUSECOOPERS,PRICEWATERHOUSE,COOPERS,AFGHANI,ARIARY,BAHT,'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.replace('\\n', ',')\n",
    "data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ef23874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Imagine speak comprehend . You captivity breathe . You finally selected happy moment . You finally freedom sadly free prodded rods fed toxic substances . The scream vain . Your voice heard lost noise '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_texts = \" \".join([i for i in token if i not in data])\n",
    "new_texts[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea392da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length before cleaning:  2113\n",
      "length after cleaning:  1156\n"
     ]
    }
   ],
   "source": [
    "print(\"length before cleaning: \", len(texts))\n",
    "print(\"length after cleaning: \", len(new_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f456147",
   "metadata": {},
   "source": [
    "# Negative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc3618a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"D:\\\\lab\\\\Dataset\\\\Negative_words.txt\", 'r') as file:\n",
    "    negative = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2123c4cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2-faced\\n2-faces\\nabnormal\\nabolish\\nabominable\\nabominably\\nabominate\\nabomination\\nabort\\naborted\\naborts\\nabrade\\nabrasive\\nabrupt\\nabruptly\\nabscond\\nabsence\\nabsent-minded\\nabsentee\\nabsurd\\nabsurdity\\nabsurdly\\nabsurdness\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative[:206]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28f44c97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2-faced,2-faces,abnormal,abolish,abominable,abominably,abominate,abomination,abort,aborted,aborts,abrade,abrasive,abrupt,abruptly,abscond,absence,absent-minded,absentee,absurd,absurdity,absurdly,absurdness,'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative = negative.replace('\\n', ',')\n",
    "negative[:206]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31a272c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['speak', 'happy', 'sadly', 'free', 'fed', 'toxic', 'scream', 'vain', 'lost', 'noise', 'writhe', 'pain', 'effect', 'pain', 'slowly', 'creeps', 'slowly', 'succumb', 'subjected', 'inhumane', 'sake', 'unethical', 'testing', 'important', 'ensure', 'product', 'safe', 'subjected', 'high', 'isolation', 'due', 'suffer', 'lot', 'stress', 'discomfort', 'testing', 'drawbacks', 'make', 'question', 'worth', 'cruelty', 'bio', 'bio', 'test', 'bio', 'excruciating', 'pain']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_texts = word_tokenize(new_texts)\n",
    "negative_words = [i for i in new_texts if i in negative]\n",
    "print(negative_words)\n",
    "len(negative_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b24f011",
   "metadata": {},
   "source": [
    "# Positive words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b003620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a+\\nabound\\nabounds\\nabundance\\nabundant\\naccessable\\naccessible\\nacclaim\\nacclaimed\\nacclamation\\naccolade\\naccolades\\naccommodative\\naccomodative\\naccomplish\\naccomplished\\naccomplishment\\naccomplishments\\naccurate\\naccurately\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"D:\\\\lab\\\\Dataset\\\\Positive_words.txt\", 'r') as file:\n",
    "    positive = file.read()\n",
    "positive[:210]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66ed38c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a+,abound,abounds,abundance,abundant,accessable,accessible,acclaim,acclaimed,acclamation,accolade,accolades,accommodative,accomodative,accomplish,accomplished,accomplishment,accomplishments,accurate,accurately,'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive = positive.replace('\\n',',')\n",
    "positive[:210]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc1387e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['happy', 'moment', 'freedom', 'free', 'noise', 'pain', 'effect', 'pain', 'knowledge', 'important', 'product', 'safe', 'high', 'large', 'question', 'worth', 'made', 'working', 'test', 'breakthrough', 'large', 'large', 'progress', 'harming', 'pain', 'protect', 'world']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_words = [i for i in new_texts if i in positive]\n",
    "print(positive_words)\n",
    "len(positive_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b59646d",
   "metadata": {},
   "source": [
    "#### positive Score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d0fbab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "source": [
    "pos_score = len(positive_words)\n",
    "print(pos_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a6cc12",
   "metadata": {},
   "source": [
    "#### Negative Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df0766cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n"
     ]
    }
   ],
   "source": [
    "neg_score=len(negative_words)\n",
    "print(neg_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84da7ab7",
   "metadata": {},
   "source": [
    "#### Polarity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21e88c2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.27"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Polarity_Score = (pos_score - neg_score)/((pos_score + neg_score) + 0.000001)\n",
    "round(Polarity_Score,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08bcb4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words after cleaning : 169\n"
     ]
    }
   ],
   "source": [
    "word_count = len(new_texts)\n",
    "print(\"number of words after cleaning :\",word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16495ba1",
   "metadata": {},
   "source": [
    "#### Subjectivity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "841713a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Subjectivity_Score = (pos_score + neg_score)/ ((word_count) + 0.000001)\n",
    "round(Subjectivity_Score,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7c370e",
   "metadata": {},
   "source": [
    "# Analysis of Readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "10209a7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "397"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "word_tokens = nltk.word_tokenize(texts)\n",
    "No_of_words = len(word_tokens)\n",
    "No_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bfac2ab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokens = nltk.sent_tokenize(texts)\n",
    "No_of_sents = len(sent_tokens)\n",
    "No_of_sents "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1576acfe",
   "metadata": {},
   "source": [
    "### Average sentence Length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "11d6a7ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.89"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Avg_Sents_Length = No_of_words / No_of_sents\n",
    "round(Avg_Sents_Length,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc20ee3d",
   "metadata": {},
   "source": [
    "### Percentage of Complex words\n",
    "Complex words: words with more than 2 syllable are called complex words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eba6a8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of complex words: 50\n",
      "Total number of words: 2113\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import cmudict\n",
    "cmud = cmudict.dict()\n",
    "\n",
    "# Defining a function to count syllables in a word\n",
    "def count_syllables(word):\n",
    "    phonemes = cmud[word.lower()][0] \n",
    "    return len([s for s in phonemes if s[-1].isdigit()])\n",
    "\n",
    "# Identifing complex words\n",
    "w_tokens = [i for i in word_tokens if i in cmud]\n",
    "complex_words = [word for word in w_tokens if count_syllables(word) > 2]\n",
    "\n",
    "# Calculatingnumber of complex words\n",
    "num_complex_words = len(complex_words)\n",
    "\n",
    "print(\"Number of complex words:\", num_complex_words)\n",
    "print(\"Total number of words:\", len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e982c0a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42.26"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Perc_of_Complex_words = len(texts) / num_complex_words\n",
    "round(Perc_of_Complex_words,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e948d1",
   "metadata": {},
   "source": [
    "### Fog Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "95fb10bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sentence length: 19.0\n"
     ]
    }
   ],
   "source": [
    "# Tokenize each sentence into words\n",
    "words = [word_tokens for sentence in sent_tokens]\n",
    "\n",
    "# Calculate the average sentence length\n",
    "avg_sent_len = sum(No_of_sents for sentence in words) / No_of_sents\n",
    "\n",
    "print(\"Average sentence length:\", avg_sent_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "38f5a93b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.5"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Fog_index = 0.4 * (avg_sent_len + Perc_of_Complex_words)\n",
    "round(Fog_index,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4aacea",
   "metadata": {},
   "source": [
    "# Average Number of Words Per Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "45da1407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.89"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_no_of_words_per_sent = No_of_words / No_of_sents\n",
    "round(avg_no_of_words_per_sent,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d9b493",
   "metadata": {},
   "source": [
    "# Complex Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "75304424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complex word count: 50\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import cmudict\n",
    "cmud = cmudict.dict()\n",
    "\n",
    "# Defining a function to count syllables in a word\n",
    "def count_syllables(word):\n",
    "    phonemes = cmud[word.lower()][0] \n",
    "    return len([s for s in phonemes if s[-1].isdigit()])\n",
    "\n",
    "# Identifing complex words\n",
    "w_tokens = [i for i in word_tokens if i in cmud]\n",
    "complex_words = [word for word in w_tokens if count_syllables(word) > 2]\n",
    "\n",
    "# Calculatingnumber of complex words\n",
    "num_complex_words = len(complex_words)\n",
    "\n",
    "print(\"complex word count:\", num_complex_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ea5188",
   "metadata": {},
   "source": [
    "# Word Count\n",
    "number of words after cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b4fb0c72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "169"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8076ff",
   "metadata": {},
   "source": [
    "# Syllable Count Per Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6f0b4e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a word to get syllable: humans\n",
      "Number of syllable in a word : 2\n"
     ]
    }
   ],
   "source": [
    "word = input(\"Enter a word to get syllable: \")\n",
    "def count_syllables(word):\n",
    "    phonemes = cmud[word.lower()][0] \n",
    "    return len([s for s in phonemes if s[-1].isdigit()])\n",
    "\n",
    "print(\"Number of syllable in a word :\",count_syllables(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3a18f6",
   "metadata": {},
   "source": [
    "# Personal Pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0b8e0d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Personal pronoun frequency: {'you': 10, 'it': 10, 'they': 1, 'us': 1, 'we': 5, 'them': 3}\n",
      "total number of pronouns in a article : 30\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Define a regex pattern to match personal pronouns\n",
    "pattern = r'\\b(I|you|he|she|it|we|they|me|him|her|us|them)\\b'\n",
    "\n",
    "# Count the frequency of personal pronouns\n",
    "pronoun_freq = {}\n",
    "for pronoun in re.findall(pattern, texts, re.IGNORECASE):\n",
    "    pronoun = pronoun.lower()\n",
    "    if pronoun in pronoun_freq:\n",
    "        pronoun_freq[pronoun] += 1\n",
    "    else:\n",
    "        pronoun_freq[pronoun] = 1\n",
    "\n",
    "print(\"Personal pronoun frequency:\", pronoun_freq)\n",
    "def returnSum(dict):\n",
    " \n",
    "    sum = 0\n",
    "    for i in pronoun_freq.values():\n",
    "        sum = sum + i\n",
    " \n",
    "    return sum\n",
    "print(\"total number of pronouns in a article :\",returnSum(pronoun_freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fc87f8",
   "metadata": {},
   "source": [
    "# Average Word Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5bde37d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average word length: 4.35\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total number of characters in all words\n",
    "total_chars = sum(len(word) for word in word_tokens)\n",
    "\n",
    "avg_word_length = total_chars / No_of_words\n",
    "\n",
    "print(\"Average word length:\", round(avg_word_length,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dc25ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819012c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
