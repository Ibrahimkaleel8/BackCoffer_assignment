{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9312f26",
   "metadata": {},
   "source": [
    "# Data scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef4785ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac2f2459",
   "metadata": {},
   "outputs": [],
   "source": [
    "req = requests.get(\"https://insights.blackcoffer.com/what-is-the-chance-homo-sapiens-will-survive-for-the-next-500-years/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45f9fa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(req.content, \"html.parser\")\n",
    "res = soup.title\n",
    "paras = soup.find_all('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbb256b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We’ve really done it this year. Like an insatiable glutton, the law of averages has come home to roost. We should’ve taken the hint when on the 1st of January, 66 people lost their lives in the Jakarta floods. What followed was like the highlights reel of a disaster movie franchise – a volcanic eruption in The Philippines, irrepressible bushfires in Australia, earthquakes in Russia, Iran, Turkey, India, and China. And speaking of China. 2020 has brought home the fragile mortality of the human race into sharp focus. As global Covid-19 deaths stoutly push past the grim 1 million marks, we have no choice but to question our place in the universe – are we the all-conquering masters of our domain, or mere tourists in a ruthlessly apathetic ecosystem? Is the human race on the ubiquitous three-part literary arc that defines every story, every life, every civilization – ascent, apex, and descent? Maybe when Michael Jackson unveiled his moonwalk in 1983, or when Barack Obama stepped into the White House as President of the United States in 2008, or indeed when MS Dhoni lifted the Cricket World Cup in 2011, we peaked, as a species, and everything since then has been a steady unraveling. 500 years is a long time. For context, the world population in 1500 AD was a mere 461 million. The 16-fold explosion since then is unprecedented in history, but we might just be at the tip of an iceberg. Though fertility rates are dropping and more and more people are foregoing the chance to have babies, we might just have crossed the threshold – the population projections for the year 2050 is 9.8 billion, and for 2100 is a whopping 11.2 billion1. Somewhere out there, Malthus is cackling in his grave. The year 2500 suddenly seems a long way off, and this conversation seems ever-more pertinent today. The Bulletin of Atomic Scientists is not optimistic – the famed Doomsday Clock they maintain is the closest to ‘midnight’ (our proximity to global catastrophe), since its inception in 1947. Global warming? Check. The threat of nuclear war? Check. Ongoing pandemic? Check, check, check. And yet, hope floats, for three reasons. Mankind may just have its back to the wall right now, but there are three shoots of potential that might just help us make it to 2500 AD – the advent of a basket of disruptive technologies (artificial intelligence, bio-enhancement, genetic engineering), the private sector focus on space exploration and terraforming, and good old fashioned human resilience. While the first two factors will no doubt be critical to human survival, it is the third one that we must pin our hopes on – our long-demonstrated history of surviving whatever nature, the universe, or our own self-destructive tendency, throws our way. The Next Superman? In the 13th century, in the Italian town of Pisa, an enterprising tinkerer developed the first eyeglasses, for a local friar with weakening eyesight2. All of a sudden, there existed an external device that could amplify our senses, a tool that gave us an advantage in survival. Today, LASIK surgeries obviate the need for eyeglasses entirely. Hearing aids give the gift of auditory perception back to those who had gotten used to a world of muffled voices and unheard sounds. The iPhone routinely comes with an augmented reality tool that allows us to measure the length of objects in front of us. But the real science starts where the imagination ends – augmented reality glasses, smart wearables, and virtual reality tools will be ubiquitous in the next few decades. But what after that? Science has the answer, and it’s both thrilling and scary. Artificial intelligence has become the stuff of fable, the filler for all questions left unanswered. But AI, combined with bio-enhancement and genetic engineering, might just lead to the evolution of what some are calling Homo nouveau3. Homo nouveau will be smarter, faster, more agile, and better equipped to adapt to what promises to be a world that is VUCA beyond our imaginations. What might such a human being look like? They might have a small chip embedded in their brain that utilizes artificial intelligence for enhanced sensory perception. What does that mean? It means that they would be able to see better, focuses their attention for longer, hears what they want to hear, and communicate the appropriate reaction to the rest of the body. Through a chemical in the blood, this AI chip would be able to demand the appropriate response from the body. Bio-augmentation of limbs and organs, internal and external, would mean that what a person can or cannot do is no longer determined at birth, but can simply be bought. All of a sudden, the average Joe can run faster than Usain Bolt, swim better than Michael Phelps, and…fly? Maybe. Genetic engineering will be the missing link. Already there are feverish conversations about a dystopian future featuring designer babies and a digital divide that simply cannot be overcome because it is inbuilt into one’s DNA. The breakthrough with CRISPR-Cas9 might just be the key to unlock the mysteries of DNA manipulation. So what if there’s no food left? Our body AI will adjust our appetite accordingly. No water? Absorb humidity from the air through specialized pores in the skin. The human being in 2500 AD may not be how we recognize one today. That may be our only shot. Galactic Dominance SpaceX, led by its mercurial leader Elon Musk, has been the leader here. The SpaceX Mars Programme is based on a very simple premise – as the earth’s closest planet in terms of distance and terrestrial conditions, Mars would be our best bet for colonization. Musk has invested billions of dollars in the Mars Space Programme and remains a fervent believer in the concept. And among tech visionaries, Musk is not alone. Jeff Bezos, the richest man in the world, owns Blue Origin, which simply aims to make spaceflight cheaper through incremental technological growth. Bezos, who took an online retailer of books and turned it into an ever-expanding behemoth, is not a man who thinks small. With more and more business leaders finding spaceflight and planetary colonization a tantalizing prospect, there will inevitably be a concerted push to developing an actual colony on another planet. And when the pull-factor to this development starts hitting diminishing returns, there will be the inevitable push factor as global warming and the possible increase in the eruption of pandemics begin to take their toll. It might just become a more feasible option for people to find an alternate home, if not on Mars, then on one of Saturn’s moons. A human colony on Mars sounds like a concept straight out of science fiction, but so did a permanent station in Antarctica until a few decades ago. Mount Everest seemed like an unapproachable summit until someone went ahead and planted a flag at the peak. Today, hundreds of people every year attempt the climb. As spaceflight becomes more reasonable, as the urge to explore supersedes the inertia of investment, we become closer to our best chance of survival – leaving planet earth behind and finding another home, perhaps one more forgiving of our follies. The Human Spirit The introduction to this article includes an illustrative list of disasters and misfortunes that have struck the global community this year. And yet, we survive. Businesses surge ahead, people adapt to a world of masks and social distancing, the world goes on. Earthquakes, tsunamis, pandemics, we’ve seen them all before and survived, and the human race, in its intrepid exploration of the world, pushes onward and upward. Technological visionaries envision a future that the rest of us cannot see, and then invest money in their vision. Gradually, what seemed unthinkable suddenly becomes real – the moon landing is the best possible example of that. Google launched Google Glass as the first augmented reality wearable technology, and suddenly we could foresee a future with smart eyewear. While the product didn’t quite catch on, that doesn’t mean other companies aren’t trying. Bio-augmentation is already a fast-developing industry, while CRISPR-Cas9, the “genetic scissors” is being held back only by regulatory bottlenecks. How long before the prospect of fiddling with the gene code becomes not a luxury but a necessity? The human spirit, the tendency to survive and thrive at all costs, will eventually win. The human race will survive to 2500, of that there is no doubt. The real question is, will the person who exists in 2500, with bionic chips and a bio-enhanced body structure and a modified genetic code, be called a Homo sapien? Or is Homo nouveau the way forward? \n"
     ]
    }
   ],
   "source": [
    "texts = \" \".join([paragraph.text.strip() for paragraph in paras])\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e79f5f60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8652"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8f1fcc",
   "metadata": {},
   "source": [
    "Removing Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5285c8a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8652"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "texts = re.sub(r'[\\'\\“\\”\\()\\%\\,\\-\\'\\’\\?\\ ]', ' ', texts)\n",
    "texts[0:200]\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fafe7be",
   "metadata": {},
   "source": [
    "# Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "089b8663",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "280876b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We',\n",
       " 've',\n",
       " 'really',\n",
       " 'done',\n",
       " 'it',\n",
       " 'this',\n",
       " 'year',\n",
       " '.',\n",
       " 'Like',\n",
       " 'an',\n",
       " 'insatiable',\n",
       " 'glutton',\n",
       " 'the',\n",
       " 'law',\n",
       " 'of',\n",
       " 'averages',\n",
       " 'has',\n",
       " 'come',\n",
       " 'home',\n",
       " 'to',\n",
       " 'roost',\n",
       " '.',\n",
       " 'We',\n",
       " 'should',\n",
       " 've',\n",
       " 'taken',\n",
       " 'the',\n",
       " 'hint',\n",
       " 'when',\n",
       " 'on',\n",
       " 'the',\n",
       " '1st',\n",
       " 'of',\n",
       " 'January',\n",
       " '66',\n",
       " 'people',\n",
       " 'lost',\n",
       " 'their',\n",
       " 'lives',\n",
       " 'in',\n",
       " 'the',\n",
       " 'Jakarta',\n",
       " 'floods',\n",
       " '.',\n",
       " 'What',\n",
       " 'followed',\n",
       " 'was',\n",
       " 'like',\n",
       " 'the',\n",
       " 'highlights',\n",
       " 'reel',\n",
       " 'of',\n",
       " 'a',\n",
       " 'disaster',\n",
       " 'movie',\n",
       " 'franchise',\n",
       " '–',\n",
       " 'a',\n",
       " 'volcanic',\n",
       " 'eruption',\n",
       " 'in',\n",
       " 'The',\n",
       " 'Philippines',\n",
       " 'irrepressible',\n",
       " 'bushfires',\n",
       " 'in',\n",
       " 'Australia',\n",
       " 'earthquakes',\n",
       " 'in',\n",
       " 'Russia',\n",
       " 'Iran',\n",
       " 'Turkey',\n",
       " 'India',\n",
       " 'and',\n",
       " 'China',\n",
       " '.',\n",
       " 'And',\n",
       " 'speaking',\n",
       " 'of',\n",
       " 'China',\n",
       " '.',\n",
       " '2020',\n",
       " 'has',\n",
       " 'brought',\n",
       " 'home',\n",
       " 'the',\n",
       " 'fragile',\n",
       " 'mortality',\n",
       " 'of',\n",
       " 'the',\n",
       " 'human',\n",
       " 'race',\n",
       " 'into',\n",
       " 'sharp',\n",
       " 'focus',\n",
       " '.',\n",
       " 'As',\n",
       " 'global',\n",
       " 'Covid',\n",
       " '19',\n",
       " 'deaths',\n",
       " 'stoutly',\n",
       " 'push',\n",
       " 'past',\n",
       " 'the',\n",
       " 'grim',\n",
       " '1',\n",
       " 'million',\n",
       " 'marks',\n",
       " 'we',\n",
       " 'have',\n",
       " 'no',\n",
       " 'choice',\n",
       " 'but',\n",
       " 'to',\n",
       " 'question',\n",
       " 'our',\n",
       " 'place',\n",
       " 'in',\n",
       " 'the',\n",
       " 'universe',\n",
       " '–',\n",
       " 'are',\n",
       " 'we',\n",
       " 'the',\n",
       " 'all',\n",
       " 'conquering',\n",
       " 'masters',\n",
       " 'of',\n",
       " 'our',\n",
       " 'domain',\n",
       " 'or',\n",
       " 'mere',\n",
       " 'tourists',\n",
       " 'in',\n",
       " 'a',\n",
       " 'ruthlessly',\n",
       " 'apathetic',\n",
       " 'ecosystem',\n",
       " 'Is',\n",
       " 'the',\n",
       " 'human',\n",
       " 'race',\n",
       " 'on',\n",
       " 'the',\n",
       " 'ubiquitous',\n",
       " 'three',\n",
       " 'part',\n",
       " 'literary',\n",
       " 'arc',\n",
       " 'that',\n",
       " 'defines',\n",
       " 'every',\n",
       " 'story',\n",
       " 'every',\n",
       " 'life',\n",
       " 'every',\n",
       " 'civilization',\n",
       " '–',\n",
       " 'ascent',\n",
       " 'apex',\n",
       " 'and',\n",
       " 'descent',\n",
       " 'Maybe',\n",
       " 'when',\n",
       " 'Michael',\n",
       " 'Jackson',\n",
       " 'unveiled',\n",
       " 'his',\n",
       " 'moonwalk',\n",
       " 'in',\n",
       " '1983',\n",
       " 'or',\n",
       " 'when',\n",
       " 'Barack',\n",
       " 'Obama',\n",
       " 'stepped',\n",
       " 'into',\n",
       " 'the',\n",
       " 'White',\n",
       " 'House',\n",
       " 'as',\n",
       " 'President',\n",
       " 'of',\n",
       " 'the',\n",
       " 'United',\n",
       " 'States',\n",
       " 'in',\n",
       " '2008',\n",
       " 'or',\n",
       " 'indeed',\n",
       " 'when',\n",
       " 'MS',\n",
       " 'Dhoni',\n",
       " 'lifted',\n",
       " 'the',\n",
       " 'Cricket',\n",
       " 'World',\n",
       " 'Cup',\n",
       " 'in',\n",
       " '2011',\n",
       " 'we',\n",
       " 'peaked',\n",
       " 'as',\n",
       " 'a',\n",
       " 'species',\n",
       " 'and',\n",
       " 'everything',\n",
       " 'since',\n",
       " 'then',\n",
       " 'has',\n",
       " 'been',\n",
       " 'a',\n",
       " 'steady',\n",
       " 'unraveling',\n",
       " '.',\n",
       " '500',\n",
       " 'years',\n",
       " 'is',\n",
       " 'a',\n",
       " 'long',\n",
       " 'time',\n",
       " '.',\n",
       " 'For',\n",
       " 'context',\n",
       " 'the',\n",
       " 'world',\n",
       " 'population',\n",
       " 'in',\n",
       " '1500',\n",
       " 'AD',\n",
       " 'was',\n",
       " 'a',\n",
       " 'mere',\n",
       " '461',\n",
       " 'million',\n",
       " '.',\n",
       " 'The',\n",
       " '16',\n",
       " 'fold',\n",
       " 'explosion',\n",
       " 'since',\n",
       " 'then',\n",
       " 'is',\n",
       " 'unprecedented',\n",
       " 'in',\n",
       " 'history',\n",
       " 'but',\n",
       " 'we',\n",
       " 'might',\n",
       " 'just',\n",
       " 'be',\n",
       " 'at',\n",
       " 'the',\n",
       " 'tip',\n",
       " 'of',\n",
       " 'an',\n",
       " 'iceberg',\n",
       " '.',\n",
       " 'Though',\n",
       " 'fertility',\n",
       " 'rates',\n",
       " 'are',\n",
       " 'dropping',\n",
       " 'and',\n",
       " 'more',\n",
       " 'and',\n",
       " 'more',\n",
       " 'people',\n",
       " 'are',\n",
       " 'foregoing',\n",
       " 'the',\n",
       " 'chance',\n",
       " 'to',\n",
       " 'have',\n",
       " 'babies',\n",
       " 'we',\n",
       " 'might',\n",
       " 'just',\n",
       " 'have',\n",
       " 'crossed',\n",
       " 'the',\n",
       " 'threshold',\n",
       " '–',\n",
       " 'the',\n",
       " 'population',\n",
       " 'projections',\n",
       " 'for',\n",
       " 'the',\n",
       " 'year',\n",
       " '2050',\n",
       " 'is',\n",
       " '9.8',\n",
       " 'billion',\n",
       " 'and',\n",
       " 'for',\n",
       " '2100',\n",
       " 'is',\n",
       " 'a',\n",
       " 'whopping',\n",
       " '11.2',\n",
       " 'billion1',\n",
       " '.',\n",
       " 'Somewhere',\n",
       " 'out',\n",
       " 'there',\n",
       " 'Malthus',\n",
       " 'is',\n",
       " 'cackling',\n",
       " 'in',\n",
       " 'his',\n",
       " 'grave',\n",
       " '.',\n",
       " 'The',\n",
       " 'year',\n",
       " '2500',\n",
       " 'suddenly',\n",
       " 'seems',\n",
       " 'a',\n",
       " 'long',\n",
       " 'way',\n",
       " 'off',\n",
       " 'and',\n",
       " 'this',\n",
       " 'conversation',\n",
       " 'seems',\n",
       " 'ever',\n",
       " 'more',\n",
       " 'pertinent',\n",
       " 'today',\n",
       " '.',\n",
       " 'The',\n",
       " 'Bulletin',\n",
       " 'of',\n",
       " 'Atomic',\n",
       " 'Scientists',\n",
       " 'is',\n",
       " 'not',\n",
       " 'optimistic',\n",
       " '–',\n",
       " 'the',\n",
       " 'famed',\n",
       " 'Doomsday',\n",
       " 'Clock',\n",
       " 'they',\n",
       " 'maintain',\n",
       " 'is',\n",
       " 'the',\n",
       " 'closest',\n",
       " 'to',\n",
       " '‘',\n",
       " 'midnight',\n",
       " 'our',\n",
       " 'proximity',\n",
       " 'to',\n",
       " 'global',\n",
       " 'catastrophe',\n",
       " 'since',\n",
       " 'its',\n",
       " 'inception',\n",
       " 'in',\n",
       " '1947',\n",
       " '.',\n",
       " 'Global',\n",
       " 'warming',\n",
       " 'Check',\n",
       " '.',\n",
       " 'The',\n",
       " 'threat',\n",
       " 'of',\n",
       " 'nuclear',\n",
       " 'war',\n",
       " 'Check',\n",
       " '.',\n",
       " 'Ongoing',\n",
       " 'pandemic',\n",
       " 'Check',\n",
       " 'check',\n",
       " 'check',\n",
       " '.',\n",
       " 'And',\n",
       " 'yet',\n",
       " 'hope',\n",
       " 'floats',\n",
       " 'for',\n",
       " 'three',\n",
       " 'reasons',\n",
       " '.',\n",
       " 'Mankind',\n",
       " 'may',\n",
       " 'just',\n",
       " 'have',\n",
       " 'its',\n",
       " 'back',\n",
       " 'to',\n",
       " 'the',\n",
       " 'wall',\n",
       " 'right',\n",
       " 'now',\n",
       " 'but',\n",
       " 'there',\n",
       " 'are',\n",
       " 'three',\n",
       " 'shoots',\n",
       " 'of',\n",
       " 'potential',\n",
       " 'that',\n",
       " 'might',\n",
       " 'just',\n",
       " 'help',\n",
       " 'us',\n",
       " 'make',\n",
       " 'it',\n",
       " 'to',\n",
       " '2500',\n",
       " 'AD',\n",
       " '–',\n",
       " 'the',\n",
       " 'advent',\n",
       " 'of',\n",
       " 'a',\n",
       " 'basket',\n",
       " 'of',\n",
       " 'disruptive',\n",
       " 'technologies',\n",
       " 'artificial',\n",
       " 'intelligence',\n",
       " 'bio',\n",
       " 'enhancement',\n",
       " 'genetic',\n",
       " 'engineering',\n",
       " 'the',\n",
       " 'private',\n",
       " 'sector',\n",
       " 'focus',\n",
       " 'on',\n",
       " 'space',\n",
       " 'exploration',\n",
       " 'and',\n",
       " 'terraforming',\n",
       " 'and',\n",
       " 'good',\n",
       " 'old',\n",
       " 'fashioned',\n",
       " 'human',\n",
       " 'resilience',\n",
       " '.',\n",
       " 'While',\n",
       " 'the',\n",
       " 'first',\n",
       " 'two',\n",
       " 'factors',\n",
       " 'will',\n",
       " 'no',\n",
       " 'doubt',\n",
       " 'be',\n",
       " 'critical',\n",
       " 'to',\n",
       " 'human',\n",
       " 'survival',\n",
       " 'it',\n",
       " 'is',\n",
       " 'the',\n",
       " 'third',\n",
       " 'one',\n",
       " 'that',\n",
       " 'we',\n",
       " 'must',\n",
       " 'pin',\n",
       " 'our',\n",
       " 'hopes',\n",
       " 'on',\n",
       " '–',\n",
       " 'our',\n",
       " 'long',\n",
       " 'demonstrated',\n",
       " 'history',\n",
       " 'of',\n",
       " 'surviving',\n",
       " 'whatever',\n",
       " 'nature',\n",
       " 'the',\n",
       " 'universe',\n",
       " 'or',\n",
       " 'our',\n",
       " 'own',\n",
       " 'self',\n",
       " 'destructive',\n",
       " 'tendency',\n",
       " 'throws',\n",
       " 'our',\n",
       " 'way',\n",
       " '.',\n",
       " 'The',\n",
       " 'Next',\n",
       " 'Superman',\n",
       " 'In',\n",
       " 'the',\n",
       " '13th',\n",
       " 'century',\n",
       " 'in',\n",
       " 'the',\n",
       " 'Italian',\n",
       " 'town',\n",
       " 'of',\n",
       " 'Pisa',\n",
       " 'an',\n",
       " 'enterprising',\n",
       " 'tinkerer',\n",
       " 'developed',\n",
       " 'the',\n",
       " 'first',\n",
       " 'eyeglasses',\n",
       " 'for',\n",
       " 'a',\n",
       " 'local',\n",
       " 'friar',\n",
       " 'with',\n",
       " 'weakening',\n",
       " 'eyesight2',\n",
       " '.',\n",
       " 'All',\n",
       " 'of',\n",
       " 'a',\n",
       " 'sudden',\n",
       " 'there',\n",
       " 'existed',\n",
       " 'an',\n",
       " 'external',\n",
       " 'device',\n",
       " 'that',\n",
       " 'could',\n",
       " 'amplify',\n",
       " 'our',\n",
       " 'senses',\n",
       " 'a',\n",
       " 'tool',\n",
       " 'that',\n",
       " 'gave',\n",
       " 'us',\n",
       " 'an',\n",
       " 'advantage',\n",
       " 'in',\n",
       " 'survival',\n",
       " '.',\n",
       " 'Today',\n",
       " 'LASIK',\n",
       " 'surgeries',\n",
       " 'obviate',\n",
       " 'the',\n",
       " 'need',\n",
       " 'for',\n",
       " 'eyeglasses',\n",
       " 'entirely',\n",
       " '.',\n",
       " 'Hearing',\n",
       " 'aids',\n",
       " 'give',\n",
       " 'the',\n",
       " 'gift',\n",
       " 'of',\n",
       " 'auditory',\n",
       " 'perception',\n",
       " 'back',\n",
       " 'to',\n",
       " 'those',\n",
       " 'who',\n",
       " 'had',\n",
       " 'gotten',\n",
       " 'used',\n",
       " 'to',\n",
       " 'a',\n",
       " 'world',\n",
       " 'of',\n",
       " 'muffled',\n",
       " 'voices',\n",
       " 'and',\n",
       " 'unheard',\n",
       " 'sounds',\n",
       " '.',\n",
       " 'The',\n",
       " 'iPhone',\n",
       " 'routinely',\n",
       " 'comes',\n",
       " 'with',\n",
       " 'an',\n",
       " 'augmented',\n",
       " 'reality',\n",
       " 'tool',\n",
       " 'that',\n",
       " 'allows',\n",
       " 'us',\n",
       " 'to',\n",
       " 'measure',\n",
       " 'the',\n",
       " 'length',\n",
       " 'of',\n",
       " 'objects',\n",
       " 'in',\n",
       " 'front',\n",
       " 'of',\n",
       " 'us',\n",
       " '.',\n",
       " 'But',\n",
       " 'the',\n",
       " 'real',\n",
       " 'science',\n",
       " 'starts',\n",
       " 'where',\n",
       " 'the',\n",
       " 'imagination',\n",
       " 'ends',\n",
       " '–',\n",
       " 'augmented',\n",
       " 'reality',\n",
       " 'glasses',\n",
       " 'smart',\n",
       " 'wearables',\n",
       " 'and',\n",
       " 'virtual',\n",
       " 'reality',\n",
       " 'tools',\n",
       " 'will',\n",
       " 'be',\n",
       " 'ubiquitous',\n",
       " 'in',\n",
       " 'the',\n",
       " 'next',\n",
       " 'few',\n",
       " 'decades',\n",
       " '.',\n",
       " 'But',\n",
       " 'what',\n",
       " 'after',\n",
       " 'that',\n",
       " 'Science',\n",
       " 'has',\n",
       " 'the',\n",
       " 'answer',\n",
       " 'and',\n",
       " 'it',\n",
       " 's',\n",
       " 'both',\n",
       " 'thrilling',\n",
       " 'and',\n",
       " 'scary',\n",
       " '.',\n",
       " 'Artificial',\n",
       " 'intelligence',\n",
       " 'has',\n",
       " 'become',\n",
       " 'the',\n",
       " 'stuff',\n",
       " 'of',\n",
       " 'fable',\n",
       " 'the',\n",
       " 'filler',\n",
       " 'for',\n",
       " 'all',\n",
       " 'questions',\n",
       " 'left',\n",
       " 'unanswered',\n",
       " '.',\n",
       " 'But',\n",
       " 'AI',\n",
       " 'combined',\n",
       " 'with',\n",
       " 'bio',\n",
       " 'enhancement',\n",
       " 'and',\n",
       " 'genetic',\n",
       " 'engineering',\n",
       " 'might',\n",
       " 'just',\n",
       " 'lead',\n",
       " 'to',\n",
       " 'the',\n",
       " 'evolution',\n",
       " 'of',\n",
       " 'what',\n",
       " 'some',\n",
       " 'are',\n",
       " 'calling',\n",
       " 'Homo',\n",
       " 'nouveau3',\n",
       " '.',\n",
       " 'Homo',\n",
       " 'nouveau',\n",
       " 'will',\n",
       " 'be',\n",
       " 'smarter',\n",
       " 'faster',\n",
       " 'more',\n",
       " 'agile',\n",
       " 'and',\n",
       " 'better',\n",
       " 'equipped',\n",
       " 'to',\n",
       " 'adapt',\n",
       " 'to',\n",
       " 'what',\n",
       " 'promises',\n",
       " 'to',\n",
       " 'be',\n",
       " 'a',\n",
       " 'world',\n",
       " 'that',\n",
       " 'is',\n",
       " 'VUCA',\n",
       " 'beyond',\n",
       " 'our',\n",
       " 'imaginations',\n",
       " '.',\n",
       " 'What',\n",
       " 'might',\n",
       " 'such',\n",
       " 'a',\n",
       " 'human',\n",
       " 'being',\n",
       " 'look',\n",
       " 'like',\n",
       " 'They',\n",
       " 'might',\n",
       " 'have',\n",
       " 'a',\n",
       " 'small',\n",
       " 'chip',\n",
       " 'embedded',\n",
       " 'in',\n",
       " 'their',\n",
       " 'brain',\n",
       " 'that',\n",
       " 'utilizes',\n",
       " 'artificial',\n",
       " 'intelligence',\n",
       " 'for',\n",
       " 'enhanced',\n",
       " 'sensory',\n",
       " 'perception',\n",
       " '.',\n",
       " 'What',\n",
       " 'does',\n",
       " 'that',\n",
       " 'mean',\n",
       " 'It',\n",
       " 'means',\n",
       " 'that',\n",
       " 'they',\n",
       " 'would',\n",
       " 'be',\n",
       " 'able',\n",
       " 'to',\n",
       " 'see',\n",
       " 'better',\n",
       " 'focuses',\n",
       " 'their',\n",
       " 'attention',\n",
       " 'for',\n",
       " 'longer',\n",
       " 'hears',\n",
       " 'what',\n",
       " 'they',\n",
       " 'want',\n",
       " 'to',\n",
       " 'hear',\n",
       " 'and',\n",
       " 'communicate',\n",
       " 'the',\n",
       " 'appropriate',\n",
       " 'reaction',\n",
       " 'to',\n",
       " 'the',\n",
       " 'rest',\n",
       " 'of',\n",
       " 'the',\n",
       " 'body',\n",
       " '.',\n",
       " 'Through',\n",
       " 'a',\n",
       " 'chemical',\n",
       " 'in',\n",
       " 'the',\n",
       " 'blood',\n",
       " 'this',\n",
       " 'AI',\n",
       " 'chip',\n",
       " 'would',\n",
       " 'be',\n",
       " 'able',\n",
       " 'to',\n",
       " 'demand',\n",
       " 'the',\n",
       " 'appropriate',\n",
       " 'response',\n",
       " 'from',\n",
       " 'the',\n",
       " 'body',\n",
       " '.',\n",
       " 'Bio',\n",
       " 'augmentation',\n",
       " 'of',\n",
       " 'limbs',\n",
       " 'and',\n",
       " 'organs',\n",
       " 'internal',\n",
       " 'and',\n",
       " 'external',\n",
       " 'would',\n",
       " 'mean',\n",
       " 'that',\n",
       " 'what',\n",
       " 'a',\n",
       " 'person',\n",
       " 'can',\n",
       " 'or',\n",
       " 'can',\n",
       " 'not',\n",
       " 'do',\n",
       " 'is',\n",
       " 'no',\n",
       " 'longer',\n",
       " 'determined',\n",
       " 'at',\n",
       " 'birth',\n",
       " 'but',\n",
       " 'can',\n",
       " 'simply',\n",
       " 'be',\n",
       " 'bought',\n",
       " '.',\n",
       " 'All',\n",
       " 'of',\n",
       " 'a',\n",
       " 'sudden',\n",
       " 'the',\n",
       " 'average',\n",
       " 'Joe',\n",
       " 'can',\n",
       " 'run',\n",
       " 'faster',\n",
       " 'than',\n",
       " 'Usain',\n",
       " 'Bolt',\n",
       " 'swim',\n",
       " 'better',\n",
       " 'than',\n",
       " 'Michael',\n",
       " 'Phelps',\n",
       " 'and…fly',\n",
       " 'Maybe',\n",
       " '.',\n",
       " 'Genetic',\n",
       " 'engineering',\n",
       " 'will',\n",
       " 'be',\n",
       " 'the',\n",
       " 'missing',\n",
       " 'link',\n",
       " '.',\n",
       " 'Already',\n",
       " 'there',\n",
       " 'are',\n",
       " 'feverish',\n",
       " 'conversations',\n",
       " 'about',\n",
       " 'a',\n",
       " 'dystopian',\n",
       " 'future',\n",
       " 'featuring',\n",
       " 'designer',\n",
       " 'babies',\n",
       " 'and',\n",
       " 'a',\n",
       " 'digital',\n",
       " 'divide',\n",
       " 'that',\n",
       " 'simply',\n",
       " 'can',\n",
       " 'not',\n",
       " 'be',\n",
       " 'overcome',\n",
       " 'because',\n",
       " 'it',\n",
       " 'is',\n",
       " 'inbuilt',\n",
       " 'into',\n",
       " 'one',\n",
       " 's',\n",
       " 'DNA',\n",
       " '.',\n",
       " 'The',\n",
       " 'breakthrough',\n",
       " 'with',\n",
       " 'CRISPR',\n",
       " 'Cas9',\n",
       " 'might',\n",
       " 'just',\n",
       " 'be',\n",
       " 'the',\n",
       " 'key',\n",
       " 'to',\n",
       " 'unlock',\n",
       " 'the',\n",
       " 'mysteries',\n",
       " 'of',\n",
       " 'DNA',\n",
       " 'manipulation',\n",
       " '.',\n",
       " 'So',\n",
       " 'what',\n",
       " 'if',\n",
       " 'there',\n",
       " 's',\n",
       " 'no',\n",
       " 'food',\n",
       " 'left',\n",
       " 'Our',\n",
       " 'body',\n",
       " 'AI',\n",
       " 'will',\n",
       " 'adjust',\n",
       " 'our',\n",
       " 'appetite',\n",
       " 'accordingly',\n",
       " '.',\n",
       " 'No',\n",
       " 'water',\n",
       " 'Absorb',\n",
       " 'humidity',\n",
       " 'from',\n",
       " 'the',\n",
       " 'air',\n",
       " 'through',\n",
       " 'specialized',\n",
       " 'pores',\n",
       " 'in',\n",
       " 'the',\n",
       " 'skin',\n",
       " '.',\n",
       " 'The',\n",
       " 'human',\n",
       " 'being',\n",
       " 'in',\n",
       " '2500',\n",
       " 'AD',\n",
       " 'may',\n",
       " 'not',\n",
       " 'be',\n",
       " 'how',\n",
       " 'we',\n",
       " 'recognize',\n",
       " 'one',\n",
       " 'today',\n",
       " '.',\n",
       " 'That',\n",
       " 'may',\n",
       " 'be',\n",
       " 'our',\n",
       " 'only',\n",
       " 'shot',\n",
       " '.',\n",
       " 'Galactic',\n",
       " 'Dominance',\n",
       " 'SpaceX',\n",
       " 'led',\n",
       " 'by',\n",
       " 'its',\n",
       " 'mercurial',\n",
       " 'leader',\n",
       " 'Elon',\n",
       " 'Musk',\n",
       " 'has',\n",
       " 'been',\n",
       " 'the',\n",
       " 'leader',\n",
       " 'here',\n",
       " '.',\n",
       " 'The',\n",
       " 'SpaceX',\n",
       " 'Mars',\n",
       " 'Programme',\n",
       " 'is',\n",
       " 'based',\n",
       " 'on',\n",
       " 'a',\n",
       " 'very',\n",
       " 'simple',\n",
       " 'premise',\n",
       " '–',\n",
       " 'as',\n",
       " 'the',\n",
       " 'earth',\n",
       " 's',\n",
       " 'closest',\n",
       " 'planet',\n",
       " 'in',\n",
       " 'terms',\n",
       " 'of',\n",
       " 'distance',\n",
       " 'and',\n",
       " ...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = word_tokenize(texts)\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10016a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"D:\\lab\\Dataset\\Stopwords_Blackcoffer.txt\", 'r') as file:\n",
    "    data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c567cbef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ERNST\\nYOUNG\\nDELOITTE\\nTOUCHE\\nKPMG\\nPRICEWATERHOUSECOOPERS\\nPRICEWATERHOUSE\\nCOOPERS\\nAFGHANI\\nARIARY\\nBAHT\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d65bfa92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ERNST,YOUNG,DELOITTE,TOUCHE,KPMG,PRICEWATERHOUSECOOPERS,PRICEWATERHOUSE,COOPERS,AFGHANI,ARIARY,BAHT,'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.replace('\\n', ',')\n",
    "data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ef23874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We year . Like insatiable glutton law averages home roost . We hint 1st January 66 people lost lives Jakarta floods . What highlights reel disaster movie franchise – volcanic eruption The Philippines '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_texts = \" \".join([i for i in token if i not in data])\n",
    "new_texts[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea392da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length before cleaning:  8652\n",
      "length after cleaning:  5472\n"
     ]
    }
   ],
   "source": [
    "print(\"length before cleaning: \", len(texts))\n",
    "print(\"length after cleaning: \", len(new_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f456147",
   "metadata": {},
   "source": [
    "# Negative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc3618a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"D:\\\\lab\\\\Dataset\\\\Negative_words.txt\", 'r') as file:\n",
    "    negative = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2123c4cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2-faced\\n2-faces\\nabnormal\\nabolish\\nabominable\\nabominably\\nabominate\\nabomination\\nabort\\naborted\\naborts\\nabrade\\nabrasive\\nabrupt\\nabruptly\\nabscond\\nabsence\\nabsent-minded\\nabsentee\\nabsurd\\nabsurdity\\nabsurdly\\nabsurdness\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative[:206]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28f44c97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2-faced,2-faces,abnormal,abolish,abominable,abominably,abominate,abomination,abort,aborted,aborts,abrade,abrasive,abrupt,abruptly,abscond,absence,absent-minded,absentee,absurd,absurdity,absurdly,absurdness,'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative = negative.replace('\\n', ',')\n",
    "negative[:206]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31a272c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['insatiable', 'law', 'hint', 'lost', 'disaster', 'eruption', 'irrepressible', 'fragile', 'human', 'race', 'sharp', 'grim', 'question', 'ruthlessly', 'apathetic', 'human', 'race', 'arc', 'life', 'steady', 'tip', 'rates', 'grave', 'pertinent', 'catastrophe', 'threat', 'check', 'check', 'back', 'wall', 'make', 'disruptive', 'bio', 'human', 'doubt', 'critical', 'human', 'pin', 'nature', 'destructive', 'throws', 'developed', 'weakening', 'advantage', 'back', 'length', 'front', 'scary', 'stuff', 'left', 'bio', 'lead', 'agile', 'human', 'small', 'brain', 'hear', 'reaction', 'rest', 'blood', 'person', 'determined', 'run', 'feverish', 'key', 'manipulation', 'left', 'adjust', 'water', 'air', 'human', 'led', 'earth', 'believer', 'concept', 'owns', 'make', 'small', 'inevitably', 'concerted', 'factor', 'inevitable', 'factor', 'eruption', 'toll', 'human', 'concept', 'fiction', 'station', 'ago', 'flag', 'peak', 'reasonable', 'urge', 'earth', 'forgiving', 'list', 'struck', 'surge', 'social', 'human', 'race', 'rest', 'unthinkable', 'foresee', 'product', 'fast', 'back', 'gene', 'human', 'spirit', 'human', 'race', 'doubt', 'question', 'person', 'bio']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_texts = word_tokenize(new_texts)\n",
    "negative_words = [i for i in new_texts if i in negative]\n",
    "print(negative_words)\n",
    "len(negative_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b24f011",
   "metadata": {},
   "source": [
    "# Positive words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b003620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a+\\nabound\\nabounds\\nabundance\\nabundant\\naccessable\\naccessible\\nacclaim\\nacclaimed\\nacclamation\\naccolade\\naccolades\\naccommodative\\naccomodative\\naccomplish\\naccomplished\\naccomplishment\\naccomplishments\\naccurate\\naccurately\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"D:\\\\lab\\\\Dataset\\\\Positive_words.txt\", 'r') as file:\n",
    "    positive = file.read()\n",
    "positive[:210]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66ed38c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a+,abound,abounds,abundance,abundant,accessable,accessible,acclaim,acclaimed,acclamation,accolade,accolades,accommodative,accomodative,accomplish,accomplished,accomplishment,accomplishments,accurate,accurately,'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive = positive.replace('\\n',',')\n",
    "positive[:210]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc1387e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['law', 'human', 'race', 'sharp', 'marks', 'question', 'masters', 'human', 'race', 'life', 'steady', 'world', 'dropping', 'optimistic', 'famed', 'warming', 'back', 'advent', 'intelligence', 'enhancement', 'good', 'human', 'doubt', 'human', 'survival', 'pin', 'enterprising', 'advantage', 'survival', 'gift', 'back', 'world', 'smart', 'thrilling', 'intelligence', 'fable', 'enhancement', 'lead', 'evolution', 'smarter', 'faster', 'agile', 'adapt', 'promises', 'world', 'human', 'brain', 'intelligence', 'enhanced', 'hear', 'rest', 'person', 'run', 'faster', 'future', 'breakthrough', 'adjust', 'air', 'human', 'led', 'simple', 'fervent', 'world', 'cheaper', 'tantalizing', 'factor', 'factor', 'warming', 'toll', 'feasible', 'human', 'straight', 'reasonable', 'survival', 'list', 'struck', 'adapt', 'world', 'world', 'human', 'race', 'world', 'future', 'rest', 'vision', 'future', 'smart', 'product', 'catch', 'fast', 'held', 'back', 'gene', 'luxury', 'human', 'spirit', 'thrive', 'human', 'race', 'doubt', 'question', 'person', 'enhanced', 'structure', 'forward']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_words = [i for i in new_texts if i in positive]\n",
    "print(positive_words)\n",
    "len(positive_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b59646d",
   "metadata": {},
   "source": [
    "#### positive Score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d0fbab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105\n"
     ]
    }
   ],
   "source": [
    "pos_score = len(positive_words)\n",
    "print(pos_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a6cc12",
   "metadata": {},
   "source": [
    "#### Negative Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df0766cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117\n"
     ]
    }
   ],
   "source": [
    "neg_score=len(negative_words)\n",
    "print(neg_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84da7ab7",
   "metadata": {},
   "source": [
    "#### Polarity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21e88c2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.05"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Polarity_Score = (pos_score - neg_score)/((pos_score + neg_score) + 0.000001)\n",
    "round(Polarity_Score,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08bcb4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words after cleaning : 790\n"
     ]
    }
   ],
   "source": [
    "word_count = len(new_texts)\n",
    "print(\"number of words after cleaning :\",word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16495ba1",
   "metadata": {},
   "source": [
    "#### Subjectivity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "841713a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Subjectivity_Score = (pos_score + neg_score)/ ((word_count) + 0.000001)\n",
    "round(Subjectivity_Score,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7c370e",
   "metadata": {},
   "source": [
    "# Analysis of Readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "10209a7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1542"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "word_tokens = nltk.word_tokenize(texts)\n",
    "No_of_words = len(word_tokens)\n",
    "No_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bfac2ab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokens = nltk.sent_tokenize(texts)\n",
    "No_of_sents = len(sent_tokens)\n",
    "No_of_sents "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1576acfe",
   "metadata": {},
   "source": [
    "### Average sentence Length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "11d6a7ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.01"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Avg_Sents_Length = No_of_words / No_of_sents\n",
    "round(Avg_Sents_Length,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc20ee3d",
   "metadata": {},
   "source": [
    "### Percentage of Complex words\n",
    "Complex words: words with more than 2 syllable are called complex words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eba6a8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of complex words: 195\n",
      "Total number of words: 8652\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import cmudict\n",
    "cmud = cmudict.dict()\n",
    "\n",
    "# Defining a function to count syllables in a word\n",
    "def count_syllables(word):\n",
    "    phonemes = cmud[word.lower()][0] \n",
    "    return len([s for s in phonemes if s[-1].isdigit()])\n",
    "\n",
    "# Identifing complex words\n",
    "w_tokens = [i for i in word_tokens if i in cmud]\n",
    "complex_words = [word for word in w_tokens if count_syllables(word) > 2]\n",
    "\n",
    "# Calculatingnumber of complex words\n",
    "num_complex_words = len(complex_words)\n",
    "\n",
    "print(\"Number of complex words:\", num_complex_words)\n",
    "print(\"Total number of words:\", len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e982c0a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44.37"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Perc_of_Complex_words = len(texts) / num_complex_words\n",
    "round(Perc_of_Complex_words,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e948d1",
   "metadata": {},
   "source": [
    "### Fog Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "95fb10bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sentence length: 67.0\n"
     ]
    }
   ],
   "source": [
    "# Tokenize each sentence into words\n",
    "words = [word_tokens for sentence in sent_tokens]\n",
    "\n",
    "# Calculate the average sentence length\n",
    "avg_sent_len = sum(No_of_sents for sentence in words) / No_of_sents\n",
    "\n",
    "print(\"Average sentence length:\", avg_sent_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "38f5a93b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44.55"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Fog_index = 0.4 * (avg_sent_len + Perc_of_Complex_words)\n",
    "round(Fog_index,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4aacea",
   "metadata": {},
   "source": [
    "# Average Number of Words Per Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "45da1407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.01"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_no_of_words_per_sent = No_of_words / No_of_sents\n",
    "round(avg_no_of_words_per_sent,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d9b493",
   "metadata": {},
   "source": [
    "# Complex Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "75304424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complex word count: 195\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import cmudict\n",
    "cmud = cmudict.dict()\n",
    "\n",
    "# Defining a function to count syllables in a word\n",
    "def count_syllables(word):\n",
    "    phonemes = cmud[word.lower()][0] \n",
    "    return len([s for s in phonemes if s[-1].isdigit()])\n",
    "\n",
    "# Identifing complex words\n",
    "w_tokens = [i for i in word_tokens if i in cmud]\n",
    "complex_words = [word for word in w_tokens if count_syllables(word) > 2]\n",
    "\n",
    "# Calculatingnumber of complex words\n",
    "num_complex_words = len(complex_words)\n",
    "\n",
    "print(\"complex word count:\", num_complex_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ea5188",
   "metadata": {},
   "source": [
    "# Word Count\n",
    "number of words after cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b4fb0c72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "790"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8076ff",
   "metadata": {},
   "source": [
    "# Syllable Count Per Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6f0b4e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a word to get syllable: question\n",
      "Number of syllable in a word : 2\n"
     ]
    }
   ],
   "source": [
    "word = input(\"Enter a word to get syllable: \")\n",
    "def count_syllables(word):\n",
    "    phonemes = cmud[word.lower()][0] \n",
    "    return len([s for s in phonemes if s[-1].isdigit()])\n",
    "\n",
    "print(\"Number of syllable in a word :\",count_syllables(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3a18f6",
   "metadata": {},
   "source": [
    "# Personal Pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0b8e0d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Personal pronoun frequency: {'we': 13, 'it': 8, 'they': 4, 'us': 5, 'them': 1}\n",
      "total number of pronouns in a article : 31\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Define a regex pattern to match personal pronouns\n",
    "pattern = r'\\b(I|you|he|she|it|we|they|me|him|her|us|them)\\b'\n",
    "\n",
    "# Count the frequency of personal pronouns\n",
    "pronoun_freq = {}\n",
    "for pronoun in re.findall(pattern, texts, re.IGNORECASE):\n",
    "    pronoun = pronoun.lower()\n",
    "    if pronoun in pronoun_freq:\n",
    "        pronoun_freq[pronoun] += 1\n",
    "    else:\n",
    "        pronoun_freq[pronoun] = 1\n",
    "\n",
    "print(\"Personal pronoun frequency:\", pronoun_freq)\n",
    "def returnSum(dict):\n",
    " \n",
    "    sum = 0\n",
    "    for i in pronoun_freq.values():\n",
    "        sum = sum + i\n",
    " \n",
    "    return sum\n",
    "print(\"total number of pronouns in a article :\",returnSum(pronoun_freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fc87f8",
   "metadata": {},
   "source": [
    "# Average Word Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5bde37d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average word length: 4.58\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total number of characters in all words\n",
    "total_chars = sum(len(word) for word in word_tokens)\n",
    "\n",
    "avg_word_length = total_chars / No_of_words\n",
    "\n",
    "print(\"Average word length:\", round(avg_word_length,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dc25ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819012c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
