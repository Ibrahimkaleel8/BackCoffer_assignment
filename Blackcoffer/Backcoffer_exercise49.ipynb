{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9312f26",
   "metadata": {},
   "source": [
    "# Data scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef4785ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac2f2459",
   "metadata": {},
   "outputs": [],
   "source": [
    "req = requests.get(\"https://insights.blackcoffer.com/should-people-wear-fabric-gloves-seeking-evidence-regarding-the-differential-transfer-of-covid-19-or-coronaviruses-generally-between-surfaces/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45f9fa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(req.content, \"html.parser\")\n",
    "res = soup.title\n",
    "paras = soup.find_all('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbb256b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is there any evidence about whether fabric or bare hands would spread covid-19 more from one surface to another? (e.g. opening a door using a fabric glove). Assuming the fabric doesn’t touch your face, like bare hands easily do, even with efforts to remember not to people struggle to remember. I’ve been thinking of prompts to prevent face touching, such as wearing fabric gloves when out and about, but I don’t want to promote this unless the fabric is at-least-no-worse than hands for transferring the virus between surfaces. I’m talking about use in the community for instance when people have to travel or go to food shops, not in a healthcare context. The gloves should be changed between different settings (e.g. apartment block, bus, shop – switch to a new pair of gloves for each). The gloves should be ironed hot at the end of each day. As far as I can tell from my attempts searching, this is evidence-based, but I’m keen to know from someone who is better at interpreting evidence in this field if it is sensible. Use fabric gloves to avoid spreading of Coronavirus from outside every person going out for daily work touching every were using these gloves before entering the house remove the glove outside the home and clean hand with sanitizer to kill the virus.  Use of this glove to wash with sanitizer and bowling in water about 60 to 70c is put outside the home for drying in night time morning these gloves are used. Any time you go outside and come back come do this is the better way to avoid spreading of Coronavirus. Well, the risk of conducting covid19 by using gloves or not the same but the gloves remember us not to touch our eyes, mouth, and nose so don’t get infection properly if you contact an infected patient, at that time wearing gloves become a must.\n"
     ]
    }
   ],
   "source": [
    "texts = \" \".join([paragraph.text.strip() for paragraph in paras])\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e79f5f60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1785"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8f1fcc",
   "metadata": {},
   "source": [
    "Removing Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5285c8a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1785"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "texts = re.sub(r'[\\'\\“\\”\\()\\%\\,\\-\\'\\’\\?\\ ]', ' ', texts)\n",
    "texts[0:200]\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fafe7be",
   "metadata": {},
   "source": [
    "# Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "089b8663",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "280876b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Is',\n",
       " 'there',\n",
       " 'any',\n",
       " 'evidence',\n",
       " 'about',\n",
       " 'whether',\n",
       " 'fabric',\n",
       " 'or',\n",
       " 'bare',\n",
       " 'hands',\n",
       " 'would',\n",
       " 'spread',\n",
       " 'covid',\n",
       " '19',\n",
       " 'more',\n",
       " 'from',\n",
       " 'one',\n",
       " 'surface',\n",
       " 'to',\n",
       " 'another',\n",
       " 'e.g',\n",
       " '.',\n",
       " 'opening',\n",
       " 'a',\n",
       " 'door',\n",
       " 'using',\n",
       " 'a',\n",
       " 'fabric',\n",
       " 'glove',\n",
       " '.',\n",
       " 'Assuming',\n",
       " 'the',\n",
       " 'fabric',\n",
       " 'doesn',\n",
       " 't',\n",
       " 'touch',\n",
       " 'your',\n",
       " 'face',\n",
       " 'like',\n",
       " 'bare',\n",
       " 'hands',\n",
       " 'easily',\n",
       " 'do',\n",
       " 'even',\n",
       " 'with',\n",
       " 'efforts',\n",
       " 'to',\n",
       " 'remember',\n",
       " 'not',\n",
       " 'to',\n",
       " 'people',\n",
       " 'struggle',\n",
       " 'to',\n",
       " 'remember',\n",
       " '.',\n",
       " 'I',\n",
       " 've',\n",
       " 'been',\n",
       " 'thinking',\n",
       " 'of',\n",
       " 'prompts',\n",
       " 'to',\n",
       " 'prevent',\n",
       " 'face',\n",
       " 'touching',\n",
       " 'such',\n",
       " 'as',\n",
       " 'wearing',\n",
       " 'fabric',\n",
       " 'gloves',\n",
       " 'when',\n",
       " 'out',\n",
       " 'and',\n",
       " 'about',\n",
       " 'but',\n",
       " 'I',\n",
       " 'don',\n",
       " 't',\n",
       " 'want',\n",
       " 'to',\n",
       " 'promote',\n",
       " 'this',\n",
       " 'unless',\n",
       " 'the',\n",
       " 'fabric',\n",
       " 'is',\n",
       " 'at',\n",
       " 'least',\n",
       " 'no',\n",
       " 'worse',\n",
       " 'than',\n",
       " 'hands',\n",
       " 'for',\n",
       " 'transferring',\n",
       " 'the',\n",
       " 'virus',\n",
       " 'between',\n",
       " 'surfaces',\n",
       " '.',\n",
       " 'I',\n",
       " 'm',\n",
       " 'talking',\n",
       " 'about',\n",
       " 'use',\n",
       " 'in',\n",
       " 'the',\n",
       " 'community',\n",
       " 'for',\n",
       " 'instance',\n",
       " 'when',\n",
       " 'people',\n",
       " 'have',\n",
       " 'to',\n",
       " 'travel',\n",
       " 'or',\n",
       " 'go',\n",
       " 'to',\n",
       " 'food',\n",
       " 'shops',\n",
       " 'not',\n",
       " 'in',\n",
       " 'a',\n",
       " 'healthcare',\n",
       " 'context',\n",
       " '.',\n",
       " 'The',\n",
       " 'gloves',\n",
       " 'should',\n",
       " 'be',\n",
       " 'changed',\n",
       " 'between',\n",
       " 'different',\n",
       " 'settings',\n",
       " 'e.g',\n",
       " '.',\n",
       " 'apartment',\n",
       " 'block',\n",
       " 'bus',\n",
       " 'shop',\n",
       " '–',\n",
       " 'switch',\n",
       " 'to',\n",
       " 'a',\n",
       " 'new',\n",
       " 'pair',\n",
       " 'of',\n",
       " 'gloves',\n",
       " 'for',\n",
       " 'each',\n",
       " '.',\n",
       " 'The',\n",
       " 'gloves',\n",
       " 'should',\n",
       " 'be',\n",
       " 'ironed',\n",
       " 'hot',\n",
       " 'at',\n",
       " 'the',\n",
       " 'end',\n",
       " 'of',\n",
       " 'each',\n",
       " 'day',\n",
       " '.',\n",
       " 'As',\n",
       " 'far',\n",
       " 'as',\n",
       " 'I',\n",
       " 'can',\n",
       " 'tell',\n",
       " 'from',\n",
       " 'my',\n",
       " 'attempts',\n",
       " 'searching',\n",
       " 'this',\n",
       " 'is',\n",
       " 'evidence',\n",
       " 'based',\n",
       " 'but',\n",
       " 'I',\n",
       " 'm',\n",
       " 'keen',\n",
       " 'to',\n",
       " 'know',\n",
       " 'from',\n",
       " 'someone',\n",
       " 'who',\n",
       " 'is',\n",
       " 'better',\n",
       " 'at',\n",
       " 'interpreting',\n",
       " 'evidence',\n",
       " 'in',\n",
       " 'this',\n",
       " 'field',\n",
       " 'if',\n",
       " 'it',\n",
       " 'is',\n",
       " 'sensible',\n",
       " '.',\n",
       " 'Use',\n",
       " 'fabric',\n",
       " 'gloves',\n",
       " 'to',\n",
       " 'avoid',\n",
       " 'spreading',\n",
       " 'of',\n",
       " 'Coronavirus',\n",
       " 'from',\n",
       " 'outside',\n",
       " 'every',\n",
       " 'person',\n",
       " 'going',\n",
       " 'out',\n",
       " 'for',\n",
       " 'daily',\n",
       " 'work',\n",
       " 'touching',\n",
       " 'every',\n",
       " 'were',\n",
       " 'using',\n",
       " 'these',\n",
       " 'gloves',\n",
       " 'before',\n",
       " 'entering',\n",
       " 'the',\n",
       " 'house',\n",
       " 'remove',\n",
       " 'the',\n",
       " 'glove',\n",
       " 'outside',\n",
       " 'the',\n",
       " 'home',\n",
       " 'and',\n",
       " 'clean',\n",
       " 'hand',\n",
       " 'with',\n",
       " 'sanitizer',\n",
       " 'to',\n",
       " 'kill',\n",
       " 'the',\n",
       " 'virus',\n",
       " '.',\n",
       " 'Use',\n",
       " 'of',\n",
       " 'this',\n",
       " 'glove',\n",
       " 'to',\n",
       " 'wash',\n",
       " 'with',\n",
       " 'sanitizer',\n",
       " 'and',\n",
       " 'bowling',\n",
       " 'in',\n",
       " 'water',\n",
       " 'about',\n",
       " '60',\n",
       " 'to',\n",
       " '70c',\n",
       " 'is',\n",
       " 'put',\n",
       " 'outside',\n",
       " 'the',\n",
       " 'home',\n",
       " 'for',\n",
       " 'drying',\n",
       " 'in',\n",
       " 'night',\n",
       " 'time',\n",
       " 'morning',\n",
       " 'these',\n",
       " 'gloves',\n",
       " 'are',\n",
       " 'used',\n",
       " '.',\n",
       " 'Any',\n",
       " 'time',\n",
       " 'you',\n",
       " 'go',\n",
       " 'outside',\n",
       " 'and',\n",
       " 'come',\n",
       " 'back',\n",
       " 'come',\n",
       " 'do',\n",
       " 'this',\n",
       " 'is',\n",
       " 'the',\n",
       " 'better',\n",
       " 'way',\n",
       " 'to',\n",
       " 'avoid',\n",
       " 'spreading',\n",
       " 'of',\n",
       " 'Coronavirus',\n",
       " '.',\n",
       " 'Well',\n",
       " 'the',\n",
       " 'risk',\n",
       " 'of',\n",
       " 'conducting',\n",
       " 'covid19',\n",
       " 'by',\n",
       " 'using',\n",
       " 'gloves',\n",
       " 'or',\n",
       " 'not',\n",
       " 'the',\n",
       " 'same',\n",
       " 'but',\n",
       " 'the',\n",
       " 'gloves',\n",
       " 'remember',\n",
       " 'us',\n",
       " 'not',\n",
       " 'to',\n",
       " 'touch',\n",
       " 'our',\n",
       " 'eyes',\n",
       " 'mouth',\n",
       " 'and',\n",
       " 'nose',\n",
       " 'so',\n",
       " 'don',\n",
       " 't',\n",
       " 'get',\n",
       " 'infection',\n",
       " 'properly',\n",
       " 'if',\n",
       " 'you',\n",
       " 'contact',\n",
       " 'an',\n",
       " 'infected',\n",
       " 'patient',\n",
       " 'at',\n",
       " 'that',\n",
       " 'time',\n",
       " 'wearing',\n",
       " 'gloves',\n",
       " 'become',\n",
       " 'a',\n",
       " 'must',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = word_tokenize(texts)\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10016a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"D:\\lab\\Dataset\\Stopwords_Blackcoffer.txt\", 'r') as file:\n",
    "    data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c567cbef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ERNST\\nYOUNG\\nDELOITTE\\nTOUCHE\\nKPMG\\nPRICEWATERHOUSECOOPERS\\nPRICEWATERHOUSE\\nCOOPERS\\nAFGHANI\\nARIARY\\nBAHT\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d65bfa92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ERNST,YOUNG,DELOITTE,TOUCHE,KPMG,PRICEWATERHOUSECOOPERS,PRICEWATERHOUSE,COOPERS,AFGHANI,ARIARY,BAHT,'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.replace('\\n', ',')\n",
    "data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ef23874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Is evidence fabric bare hands spread covid 19 surface e.g . opening door fabric glove . Assuming fabric touch face bare hands easily efforts remember people struggle remember . thinking prompts preven'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_texts = \" \".join([i for i in token if i not in data])\n",
    "new_texts[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea392da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length before cleaning:  1785\n",
      "length after cleaning:  950\n"
     ]
    }
   ],
   "source": [
    "print(\"length before cleaning: \", len(texts))\n",
    "print(\"length after cleaning: \", len(new_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f456147",
   "metadata": {},
   "source": [
    "# Negative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc3618a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"D:\\\\lab\\\\Dataset\\\\Negative_words.txt\", 'r') as file:\n",
    "    negative = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2123c4cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2-faced\\n2-faces\\nabnormal\\nabolish\\nabominable\\nabominably\\nabominate\\nabomination\\nabort\\naborted\\naborts\\nabrade\\nabrasive\\nabrupt\\nabruptly\\nabscond\\nabsence\\nabsent-minded\\nabsentee\\nabsurd\\nabsurdity\\nabsurdly\\nabsurdness\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative[:206]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28f44c97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2-faced,2-faces,abnormal,abolish,abominable,abominably,abominate,abomination,abort,aborted,aborts,abrade,abrasive,abrupt,abruptly,abscond,absence,absent-minded,absentee,absurd,absurdity,absurdly,absurdness,'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative = negative.replace('\\n', ',')\n",
    "negative[:206]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31a272c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fabric', 'fabric', 'fabric', 'touch', 'face', 'easily', 'struggle', 'face', 'fabric', 'fabric', 'worse', 'virus', 'block', 'bus', 'pair', 'hot', 'day', 'fabric', 'avoid', 'person', 'work', 'house', 'clean', 'kill', 'virus', 'wash', 'water', 'put', 'night', 'back', 'avoid', 'risk', 'touch', 'eyes', 'nose', 'infection', 'properly', 'infected', 'patient']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_texts = word_tokenize(new_texts)\n",
    "negative_words = [i for i in new_texts if i in negative]\n",
    "print(negative_words)\n",
    "len(negative_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b24f011",
   "metadata": {},
   "source": [
    "# Positive words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b003620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a+\\nabound\\nabounds\\nabundance\\nabundant\\naccessable\\naccessible\\nacclaim\\nacclaimed\\nacclamation\\naccolade\\naccolades\\naccommodative\\naccomodative\\naccomplish\\naccomplished\\naccomplishment\\naccomplishments\\naccurate\\naccurately\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"D:\\\\lab\\\\Dataset\\\\Positive_words.txt\", 'r') as file:\n",
    "    positive = file.read()\n",
    "positive[:210]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66ed38c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a+,abound,abounds,abundance,abundant,accessable,accessible,acclaim,acclaimed,acclamation,accolade,accolades,accommodative,accomodative,accomplish,accomplished,accomplishment,accomplishments,accurate,accurately,'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive = positive.replace('\\n',',')\n",
    "positive[:210]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc1387e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hands', 'hands', 'promote', 'hands', 'block', 'bus', 'hot', 'keen', 'person', 'work', 'clean', 'kill', 'put', 'back', 'risk', 'properly', 'patient']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_words = [i for i in new_texts if i in positive]\n",
    "print(positive_words)\n",
    "len(positive_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b59646d",
   "metadata": {},
   "source": [
    "#### positive Score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d0fbab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "pos_score = len(positive_words)\n",
    "print(pos_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a6cc12",
   "metadata": {},
   "source": [
    "#### Negative Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df0766cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    }
   ],
   "source": [
    "neg_score=len(negative_words)\n",
    "print(neg_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84da7ab7",
   "metadata": {},
   "source": [
    "#### Polarity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21e88c2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.39"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Polarity_Score = (pos_score - neg_score)/((pos_score + neg_score) + 0.000001)\n",
    "round(Polarity_Score,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08bcb4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words after cleaning : 147\n"
     ]
    }
   ],
   "source": [
    "word_count = len(new_texts)\n",
    "print(\"number of words after cleaning :\",word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16495ba1",
   "metadata": {},
   "source": [
    "#### Subjectivity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "841713a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Subjectivity_Score = (pos_score + neg_score)/ ((word_count) + 0.000001)\n",
    "round(Subjectivity_Score,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7c370e",
   "metadata": {},
   "source": [
    "# Analysis of Readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "10209a7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "342"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "word_tokens = nltk.word_tokenize(texts)\n",
    "No_of_words = len(word_tokens)\n",
    "No_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bfac2ab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokens = nltk.sent_tokenize(texts)\n",
    "No_of_sents = len(sent_tokens)\n",
    "No_of_sents "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1576acfe",
   "metadata": {},
   "source": [
    "### Average sentence Length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "11d6a7ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.31"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Avg_Sents_Length = No_of_words / No_of_sents\n",
    "round(Avg_Sents_Length,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc20ee3d",
   "metadata": {},
   "source": [
    "### Percentage of Complex words\n",
    "Complex words: words with more than 2 syllable are called complex words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eba6a8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of complex words: 23\n",
      "Total number of words: 1785\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import cmudict\n",
    "cmud = cmudict.dict()\n",
    "\n",
    "# Defining a function to count syllables in a word\n",
    "def count_syllables(word):\n",
    "    phonemes = cmud[word.lower()][0] \n",
    "    return len([s for s in phonemes if s[-1].isdigit()])\n",
    "\n",
    "# Identifing complex words\n",
    "w_tokens = [i for i in word_tokens if i in cmud]\n",
    "complex_words = [word for word in w_tokens if count_syllables(word) > 2]\n",
    "\n",
    "# Calculatingnumber of complex words\n",
    "num_complex_words = len(complex_words)\n",
    "\n",
    "print(\"Number of complex words:\", num_complex_words)\n",
    "print(\"Total number of words:\", len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e982c0a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77.61"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Perc_of_Complex_words = len(texts) / num_complex_words\n",
    "round(Perc_of_Complex_words,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e948d1",
   "metadata": {},
   "source": [
    "### Fog Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "95fb10bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sentence length: 13.0\n"
     ]
    }
   ],
   "source": [
    "# Tokenize each sentence into words\n",
    "words = [word_tokens for sentence in sent_tokens]\n",
    "\n",
    "# Calculate the average sentence length\n",
    "avg_sent_len = sum(No_of_sents for sentence in words) / No_of_sents\n",
    "\n",
    "print(\"Average sentence length:\", avg_sent_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "38f5a93b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36.24"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Fog_index = 0.4 * (avg_sent_len + Perc_of_Complex_words)\n",
    "round(Fog_index,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4aacea",
   "metadata": {},
   "source": [
    "# Average Number of Words Per Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "45da1407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.31"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_no_of_words_per_sent = No_of_words / No_of_sents\n",
    "round(avg_no_of_words_per_sent,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d9b493",
   "metadata": {},
   "source": [
    "# Complex Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "75304424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complex word count: 23\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import cmudict\n",
    "cmud = cmudict.dict()\n",
    "\n",
    "# Defining a function to count syllables in a word\n",
    "def count_syllables(word):\n",
    "    phonemes = cmud[word.lower()][0] \n",
    "    return len([s for s in phonemes if s[-1].isdigit()])\n",
    "\n",
    "# Identifing complex words\n",
    "w_tokens = [i for i in word_tokens if i in cmud]\n",
    "complex_words = [word for word in w_tokens if count_syllables(word) > 2]\n",
    "\n",
    "# Calculatingnumber of complex words\n",
    "num_complex_words = len(complex_words)\n",
    "\n",
    "print(\"complex word count:\", num_complex_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ea5188",
   "metadata": {},
   "source": [
    "# Word Count\n",
    "number of words after cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b4fb0c72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8076ff",
   "metadata": {},
   "source": [
    "# Syllable Count Per Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aad9d83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a word to get syllable: gloves\n",
      "Number of syllable in a word : 1\n"
     ]
    }
   ],
   "source": [
    "word = input(\"Enter a word to get syllable: \")\n",
    "def count_syllables(word):\n",
    "    phonemes = cmud[word.lower()][0] \n",
    "    return len([s for s in phonemes if s[-1].isdigit()])\n",
    "\n",
    "print(\"Number of syllable in a word :\",count_syllables(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3a18f6",
   "metadata": {},
   "source": [
    "# Personal Pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0b8e0d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Personal pronoun frequency: {'i': 5, 'it': 1, 'you': 2, 'us': 1}\n",
      "total number of pronouns in a article : 9\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Define a regex pattern to match personal pronouns\n",
    "pattern = r'\\b(I|you|he|she|it|we|they|me|him|her|us|them)\\b'\n",
    "\n",
    "# Count the frequency of personal pronouns\n",
    "pronoun_freq = {}\n",
    "for pronoun in re.findall(pattern, texts, re.IGNORECASE):\n",
    "    pronoun = pronoun.lower()\n",
    "    if pronoun in pronoun_freq:\n",
    "        pronoun_freq[pronoun] += 1\n",
    "    else:\n",
    "        pronoun_freq[pronoun] = 1\n",
    "\n",
    "print(\"Personal pronoun frequency:\", pronoun_freq)\n",
    "def returnSum(dict):\n",
    " \n",
    "    sum = 0\n",
    "    for i in pronoun_freq.values():\n",
    "        sum = sum + i\n",
    " \n",
    "    return sum\n",
    "print(\"total number of pronouns in a article :\",returnSum(pronoun_freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fc87f8",
   "metadata": {},
   "source": [
    "# Average Word Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5bde37d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average word length: 4.2\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total number of characters in all words\n",
    "total_chars = sum(len(word) for word in word_tokens)\n",
    "\n",
    "avg_word_length = total_chars / No_of_words\n",
    "\n",
    "print(\"Average word length:\", round(avg_word_length,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dc25ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819012c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
